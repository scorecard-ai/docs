---
title: "Node.js"
description: "The Scorecard platform helps you evaluate the performance of your LLM app to ship faster with more confidence. "
---

In this quickstart we will:

- Get an API key and create a Testset
- Create an example LLM app
- Execute a script with the Scorecard SDK within our production application
- Review results in the Scorecard UI

You can use this [Observable notebook](https://observablehq.com/@twotau/scorecard-js-example) to run this tutorial client-side.

## Steps

<Steps>
  <Step title="Setup">
    First let’s create a [Scorecard account](https://app.getscorecard.ai/dashboard) and find your [Scorecard API Key](https://app.getscorecard.ai/settings). Then we’ll get a [OpenAI API Key](https://platform.openai.com/api-keys), and set these as environment variables and also install the Scorecard and OpenAI Node Libraries:

    ```sh Node install
    export SCORECARD_API_KEY="SCORECARD_API_KEY"
    export OPENAI_API_KEY="OPENAI_API_KEY"
    npm install scorecard-ai
    npm install openai
    ```
  </Step>
  <Step title="Create Testcases">
    Now let’s create and run a create_testset.js script to create a testset and add some test cases using the SDK. Test cases are a way to collect examples that you can run evaluations against and improve over time. After we create a testset, we’ll grab that testset ID to use later:

    ```js
    const { ScorecardClient } = require("scorecard-ai");
    require("dotenv").config();
    
    const scorecard = new ScorecardClient({ apiKey: process.env.SCORECARD_API_KEY });
    
    async function createTestset() {
      const testset = await scorecard.testset.create({
        name: "MMLU Demo",
        description: "Demo of a MMLU testset created via Scorecard Node SDK",
      });
    
      // Add three testcases
      await scorecard.testcase.create(testset.id, {
        userQuery:
          "The amount of access cabinet secretaries have to the president is most likely to be controlled by the",
      });
      await scorecard.testcase.create(testset.id, {
        userQuery: "The exclusionary rule was established to",
      });
      await scorecard.testcase.create(testset.id, {
        userQuery:
          "Ruled unconstitutional in 1983, the legislative veto had allowed",
      });
    
      console.log("Visit the Scorecard UI to view your Testset:");
      console.log(
        `https://app.getscorecard.ai/projects/${testset.projectId}/testsets/${testset.id}`
      );
    
      return testset;
    }
    ```
  </Step>
  <Step title="Create Test System">
    Next let’s create a function which will represent our system under test. Here we’ve created a system that will be a helpful assistant in response to our input user query.

    ```js
    const { OpenAI } = require("openai");
    const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
    
    async function answerQuery(userPrompt) {
      const chatCompletion = await openai.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [
          { role: "system", content: "You are a helpful assistant." },
          { role: "user", content: userPrompt },
        ],
      });
    
      return chatCompletion.data.choices[0].message.content;
    }
    ```
  </Step>
  <Step title="Create Metrics">
    Now that we have a system that answers questions from the MMLU dataset, let’s build a metric to understand how relevant the system responses are to our user query. Let’s go to the **Metrics** page and select “New Metric”

    <Frame caption="Scorecard UI: New Metric">
      ![Scorecard UI: New Metric](/images/python/1.png)
    </Frame>
    From here, let’s create a metric for answer relevancy:

    <Frame caption="Scorecard UI: Metric Definition">
      ![Scorecard UI: Metric Definition](/images/python/2.webp)
    </Frame>
    You can evaluate your LLM systems with one or multiple metrics. A good practice is to routinely test the LLM system with the same metrics for a specific use case. For this, Scorecard lets you define _Scoring Configs_, collection of metrics to be used to consistently evaluate LLM use cases with the same metrics. For the quick start, we will create a Scoring Config containing only the previously created Answer Relevancy metric. Let’s head over to the “Scoring Config” tab in the Scoring Lab and create this Scoring Config. Let’s grab that Scoring Config ID for later:

    <Frame caption="Scorecard UI: Scoring Config">
      ![Scorecard UI: Scoring Config](/images/python/3.webp)
    </Frame>
  </Step>
  <Step title="Create Test System">
    Now let’s use our mock system and run our Testset against it replacing the Testset id below with the Testset from before and the Scoring Config ID above:

    ```js
    async function runTests() {
      const run = await scorecard.run.create({
        testsetId, // Use the actual testset ID
        scoringConfigId, // Use the actual Scoring Config ID
      });
    
      // Run all testcases in parallel
      const testcases = await scorecard.testset.getTestcases(testsetId);
      await Promise.all(
        testcases.results.map((testcase) =>
          runTestcase(systemPrompt, run, testcase)
        )
      );
    
      // Set run status to waiting for metric scoring
      await scorecard.run.updateStatus(run.id, {
        status: "awaiting_scoring"
      });
    
      console.log("Visit the Scorecard UI to score the run:");
    
      // Use your testset's project ID here:
      console.log(`https://app.getscorecard.ai/projects/${projectId}/runs/grades/${run.id}`);
    
      return run;
    }
    ```
  </Step>
  <Step title="Run Scoring">
    Now let’s review the outputs of our execution in Scorecard and run scoring by clicking on the “Run Scoring button’.

    <Frame caption="Scorecard UI: Run Scoring">
      ![Scorecard UI: Run Scoring](/images/python/4.png)
    </Frame>
  </Step>
  <Step title="View Results">
    Finally let’s review the results in the Scorecard UI. Here you can view and understand the performance of your LLM system:

    <Frame caption="Scorecard UI: Viewing Results">
      ![Scorecard UI: Viewing Results](/images/python/5.png)
    </Frame>
  </Step>
</Steps>