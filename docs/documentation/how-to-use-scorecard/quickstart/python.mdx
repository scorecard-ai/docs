---
title: "Python"
description: "Scorecard helps you evaluate the performance of your LLM app to ship faster with more confidence! "
---

In this quickstart we will:

- Setup Scorecard
- Create a Testset
- Create an example LLM app with OpenAI
- Define the evaluation setup
- Score the LLM app with the Testset
- Review evaluation results in the Scorecard UI

## Steps

<Steps>
  <Step title="Setup">
    First let’s create a [Scorecard account](https://accounts.getscorecard.ai/sign-up?redirect_url=https%3A%2F%2Fapp.getscorecard.ai%2Fdashboard) and find the `SCORECARD_API_KEY` in the [settings](https://app.getscorecard.ai/settings). Since this example creates a simple LLM application using OpenAI, get an [OpenAI API key](https://platform.openai.com/api-keys). Set both API keys as environment variables as shown below. Additionally, install the Scorecard and OpenAI Python libraries:

    ```sh Python Setup
    export SCORECARD_API_KEY="SCORECARD_API_KEY"
    export OPENAI_API_KEY="OPENAI_API_KEY"
    pip install scorecard-ai
    pip install openai
    ```
  </Step>
  <Step title="Create a Testset and Add Testcases">
    A Testset is a collection of Testcases used to evaluate the performance of an LLM application across a variety of inputs and scenarios. A Testcase is a single input to an LLM that is used for scoring. Now let’s create a Testset and add some Testcases using the Scorecard Python SDK:

    ```py
    from scorecard.client import Scorecard
    
    client = Scorecard(
    	api_key=SCORECARD_API_KEY
    )
    
    # Create a Testset
    demo_testset = client.testset.create(
    	name="Demo Testset",
    	description="Demo Testset created via Scorecard Python SDK",
    )
    
    # Add three Testcases
    client.testcase.create(
    	testset_id=demo_testset.id,
    	user_query="The amount of access cabinet secretaries have to the president is most likely to be controlled by the"
    )
    client.testcase.create(
    	testset_id=demo_testset.id,
    	user_query="The exclusionary rule was established to"
    )
    client.testcase.create(
    	testset_id=demo_testset.id,
    	user_query="Ruled unconstitutional in 1983, the legislative veto had allowed"
    )
    
    print("Visit the Scorecard UI to view your Testset:")
    print(f"https://app.getscorecard.ai/projects/{demo_testset.project_id}/testsets/{demo_testset.id}")
    ```
  </Step>
  <Step title="Create a Simple LLM App">
    Next let’s create a simple LLM application which we will be evaluating using Scorecard. This LLM application is represented with the following function that sends a request with a user-defined input to the OpenAI API Here we’ve created a system that will be a helpful assistant in response to our input user query.

    ```py
    from openai import OpenAI
    
    def answer_query(user_topic: str) -> str:
    	client = OpenAI(api_key=OPENAI_API_KEY)
    	response = client.chat.completions.create(
      	model="gpt-4o-mini",  
      	messages=[
      		{"role": "system", "content": "You are a helpful assistant."},
      		{"role": "user", "content": user_topic},
          ]
    	)
    
    	return response.choices[0].message.content
    ```
  </Step>
  <Step title="Create Metrics">
    Now that we have a system that answers questions from the MMLU dataset, let’s build a metric to understand how relvent the system responses are to our user query. Let’s go to the [Scoring Lab](https://app.getscorecard.ai/scoring-lab) and select “New Metric”

    <Frame caption="Scorecard UI: New Metric">
      ![Scorecard UI: New Metric](/images/python/1.png)
    </Frame>
    From here let’s create a metric for answer relevency:

    <Frame caption="Scorecard UI: Metric Definition">
      ![Scorecard UI: Metric Definition](/images/python/2.webp)
    </Frame>
    You can evaluate your LLM systems with one or multiple metrics. A good practice is to routinely test the LLM system with the same metrics for a specific use case. For this, Scorecard offers to define Scoring Configs, collection of metrics to be used to consistently evaluate LLM use cases with the same metrics. For the quick start, we will create a Scoring Config just including the previously created Answer Relevancy metric. Let’s head over to the “Scoring Config” tab in the Scoring Lab and create this Scoring Config. Let’s grab that Scoring Config ID for later:

    <Frame caption="Scorecard UI: Scoring Config">
      ![Scorecard UI: Scoring Config](/images/python/3.webp)
    </Frame>
  </Step>
  <Step title="Create Test System">
    Now let’s use our mock system and run our Testset against it replacing the Testset id below with the Testset from before and the Scoring Config ID above:

    ```py
    client.run_tests(
    	input_testset_id=demo_testset.id,
    	scoring_config_id=456, # Replace with your scoring config ID
    	model_invocation=lambda prompt: answer_query(prompt),
    )
    
    print("Visit the Scorecard UI to view your Run:")
      print(f"https://app.getscorecard.ai/projects/{demo_testset.project_id}/runs/grades/{run.id}")
    ```
  </Step>
  <Step title="Run Scoring">
    Now let’s review the outputs of our execution in the Scorecard UI and run scoring by clicking on the “Run Scoring button’.

    <Frame caption="Scorecard UI: Run Scoring">
      ![Scorecard UI: Run Scoring](/images/python/4.png)
    </Frame>
  </Step>
  <Step title="View Results">
    Finally let’s review the results in the Scorecard UI. Here you can view and understand the performance of your LLM system:

    <Frame caption="Scorecard UI: Viewing Results">
      ![Scorecard UI: Viewing Results](/images/python/5.png)
    </Frame>
  </Step>
</Steps>