---
title: "Custom Endpoints"
sidebarTitle: "Endpoints"
description: "Test and evaluate any HTTP API endpoint with Scorecard"
---

import { Tabs, Tab, CodeGroup, Note, Tip, Warning } from "@mintlify/components";

## What are Custom Endpoints?

Custom Endpoints in Scorecard allow you to test and evaluate any HTTP API endpoint, whether it's your production API, a staging environment, or a local development server. You can test REST APIs, GraphQL endpoints, webhooks, or any HTTP-accessible service while validating that your endpoints return expected data structures and values. The feature also helps you monitor performance by tracking response times and reliability across deployments, and seamlessly integrates with CI/CD pipelines to automate endpoint testing as part of your deployment process.

<Note>
  Custom Endpoints are perfect for testing RAG pipelines, tool-calling agents, chatbots, and traditional APIs that power your AI applications.
</Note>

## Creating an Endpoint

Navigate to your project's **Endpoints** section to create and manage custom endpoints.

<DarkLightImage
  lightSrc="/images/endpoints/endpoint-edit-light.png"
  darkSrc="/images/endpoints/endpoint-edit-dark.png"
  caption={null}
  alt="Screenshot of the Edit Endpoint configuration modal"
/>

### Configuration Options

Each endpoint requires the following configuration:

<Tabs>
  <Tab title="Basic Settings">
    **Name**: A descriptive name for your endpoint
    
    **Description**: Document what this endpoint does and when to use it
    
    **Method**: HTTP method (GET, POST, PUT, DELETE, PATCH)
    
    **URL**: The full endpoint URL including protocol
    ```
    https://api.example.com/v1/chat/completions
    ```
  </Tab>
  
  <Tab title="Headers">
    Configure request headers including authentication:
    
    ```json
    {
      "Content-Type": "application/json",
      "Authorization": "Bearer YOUR_API_KEY",
      "X-Custom-Header": "value"
    }
    ```
    
    <Tip>
      Store sensitive values like API keys as environment variables for security.
    </Tip>
  </Tab>
  
  <Tab title="Request Body">
    For POST/PUT/PATCH requests, define your request body:
    
    ```json
    {
      "model": "gpt-4",
      "messages": [
        {
          "role": "user",
          "content": "{{input}}"
        }
      ],
      "temperature": 0.7
    }
    ```
    
    Use `{{variable}}` syntax to inject dynamic values from your test cases.
  </Tab>
  
  <Tab title="Response Path">
    Extract specific values from the response using JSONPath:
    
    ```
    $.choices[0].message.content
    ```
    
    This allows you to focus evaluation on the relevant part of the response.
  </Tab>
</Tabs>

## Using Endpoints in Test Runs

Once configured, endpoints can be used as systems in your test runs. Access them through the Kickoff Run modal's Endpoint tab:

<DarkLightImage
  lightSrc="/images/endpoints/kickoff-endpoint-light.png"
  darkSrc="/images/endpoints/kickoff-endpoint-dark.png"
  caption={null}
  alt="Screenshot of Kickoff Run modal with Endpoint tab selected"
/>

Run evaluations against your configured endpoints:

<CodeGroup>
```python Python
import scorecard

client = scorecard.Client(api_key="YOUR_API_KEY")

# Run tests against your endpoint
run = client.runs.create(
    project_id="your-project-id",
    testset_id="your-testset-id",
    system_id="your-endpoint-id",  # Your custom endpoint
    metrics=["accuracy", "latency"]
)

# Get results
results = client.runs.get(run.id)
print(f"Average latency: {results.metrics['latency'].mean}ms")
```

```javascript JavaScript
import Scorecard from '@scorecard/client';

const client = new Scorecard({ apiKey: 'YOUR_API_KEY' });

// Run tests against your endpoint
const run = await client.runs.create({
  projectId: 'your-project-id',
  testsetId: 'your-testset-id',
  systemId: 'your-endpoint-id',  // Your custom endpoint
  metrics: ['accuracy', 'latency']
});

// Get results
const results = await client.runs.get(run.id);
console.log(`Average latency: ${results.metrics.latency.mean}ms`);
```

```bash cURL
curl -X POST https://api.scorecard.com/v2/runs \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "project_id": "your-project-id",
    "testset_id": "your-testset-id",
    "system_id": "your-endpoint-id",
    "metrics": ["accuracy", "latency"]
  }'
```
</CodeGroup>

## Dynamic Variables

Endpoints support dynamic variable injection from your test cases:

### Variable Syntax
Use double curly braces to reference testcase fields:
- `{{input}}` - The input field from your testcase
- `{{context}}` - Additional context data
- `{{user_id}}` - Any custom field in your testcase

### Example Workflow

<Steps>
  <Step title="Define Testset Schema">
    Create a testset with fields that match your endpoint needs:
    ```json
    {
      "input": "string",
      "context": "string",
      "expected_output": "string"
    }
    ```
  </Step>
  
  <Step title="Configure Endpoint">
    Reference testcase fields in your endpoint configuration:
    ```json
    {
      "query": "{{input}}",
      "context": "{{context}}",
      "max_tokens": 100
    }
    ```
  </Step>
  
  <Step title="Run Evaluation">
    Scorecard automatically injects values from each testcase when calling your endpoint.
  </Step>
</Steps>

## Best Practices

### Security
<Warning>
  Never hardcode API keys or secrets directly in endpoint configurations. Use environment variables or secure credential storage.
</Warning>

### Error Handling
- Configure appropriate timeout values for slow endpoints
- Set up retry logic for transient failures
- Monitor error rates to identify endpoint issues

### Performance Optimization
- Use connection pooling for high-volume testing
- Implement rate limiting to avoid overwhelming your endpoints
- Cache responses when testing deterministic endpoints

### Testing Strategies

<Tabs>
  <Tab title="Smoke Testing">
    Quick validation that endpoints are responding:
    - Single test case per endpoint
    - Basic response validation
    - Fast feedback loop
  </Tab>
  
  <Tab title="Load Testing">
    Evaluate endpoint performance under load:
    - Multiple concurrent requests
    - Response time percentiles
    - Error rate monitoring
  </Tab>
  
  <Tab title="Regression Testing">
    Ensure changes don't break existing functionality:
    - Comprehensive test coverage
    - Compare against baseline metrics
    - Automated CI/CD integration
  </Tab>
</Tabs>

## Monitoring Endpoints

Set up monitors to continuously test your endpoints:

```python
# Create a monitor for your endpoint
monitor = client.monitors.create(
    project_id="your-project-id",
    name="API Health Check",
    system_id="your-endpoint-id",
    testset_id="health-check-testset",
    schedule="*/5 * * * *",  # Every 5 minutes
    alert_threshold=0.95  # Alert if success rate < 95%
)
```

## Common Use Cases

### RAG Pipeline Testing
Test retrieval-augmented generation endpoints to ensure:
- Correct context retrieval
- Accurate response generation
- Acceptable latency

### Chatbot Evaluation
Validate conversational AI endpoints for:
- Response relevance
- Conversation flow
- Error handling

### API Regression Testing
Ensure API changes don't break existing functionality:
- Response format validation
- Backward compatibility checks
- Performance regression detection

## Troubleshooting

### Connection Errors
- Verify the endpoint URL is accessible
- Check firewall and network settings
- Ensure SSL certificates are valid

### Authentication Issues
- Confirm API keys are correct and active
- Check token expiration
- Verify authentication header format

### Response Parsing
- Validate JSONPath expressions
- Ensure response format matches expectations
- Handle nullable fields appropriately

## Related Resources

<Card title="Systems" icon="server" href="/features/systems">
  Learn about different system types in Scorecard
</Card>

<Card title="Testsets" icon="vial" href="/features/testsets">
  Create test data for endpoint evaluation
</Card>

<Card title="Metrics" icon="chart-line" href="/features/best-in-class-metrics">
  Define custom metrics for endpoint evaluation
</Card>

<Card title="API Reference" icon="code" href="/api-reference/overview">
  Complete API documentation for endpoints
</Card>