---
title: "Monitoring"
description: "Continuously score production LLM traffic and track quality over time."
---

import { DarkLightImage } from '/snippets/dark-light-image.jsx';

<DarkLightImage
  lightSrc="/images/monitors/monitors-list.png"
  darkSrc="/images/monitors/monitors-list.png"
  caption="Monitor overview list."
  alt="Screenshot of monitor overview list."
/>

Scorecard **Monitors** periodically sample recent LLM spans, extract prompts and completions, and automatically score them with your selected metrics. The results appear where you work: on the **Traces** page (scored traces) and the **Runs** pages (history and per‑run aggregates).

## Why it matters

- Measure real production quality and safety continuously, not just in staging.
- Detect drift early and pinpoint regressions to specific topics or services.
- Close the loop between observability and evaluation with automatic scoring.
- Quantify improvements after model/prompt updates with linked runs and trends.

## If you’re coming from other tools

- **What it is**: Very similar to observability dashboards (metrics over time, traces, filtering) — with one key addition: Scorecard runs **evaluations/auto‑scoring** on sampled traces, so you get quality metrics over time, not just system metrics.
- **Where scores show up**: Inline on **Traces** for each scored span, and in **Runs** where you can analyze run‑level aggregates and trends.
- **What’s evaluated**: Only spans that contain prompt and completion are scored; we support common keys (`openinference.*`, `ai.prompt`/`ai.response`, `gen_ai.*`).

<DarkLightImage
  lightSrc="/images/monitors/monitors-traces-search.png"
  darkSrc="/images/monitors/monitors-traces-search.png"
  caption="Traces search page with scores created by a 'monitor'."
  alt="Screenshot of traces page."
/>

## Create a Monitor

1. In the project sidebar select **Monitors** (<code>&lt;Icons.clock/&gt;</code>). You’ll land on the monitor overview page.
2. Click **“New Monitor +”** to open the Create Monitor modal.
3. Click **Create Monitor** and scoring starts on the next cycle.
   
<DarkLightImage
  lightSrc="/images/monitors/monitor-create.png"
  darkSrc="/images/monitors/monitor-create.png"
  caption="Create monitor modal."
  alt="Create monitor modal screenshot."
/>

Inside the modal you can configure:

* **Metrics** – choose any evaluation metric you’ve defined (toxicity, factuality, latency…).
* **Frequency** – how often Scorecard samples traces (1 min, 5 min, 30 min, 1 h, 1 day).
* **Sample Rate** – throttle evaluation cost (1 %–100 %).
* **Filters** – hone in on traffic via <code>spanName</code>, <code>serviceName</code>, or full-text **searchText**.
* **Active** – flip a switch to pause / resume without losing config.

<DarkLightImage
  lightSrc="/images/monitors/monitor-metrics.png"
  darkSrc="/images/monitors/monitor-metrics.png"
  caption="Monitor options – Metrics"
  alt="Select metrics UI."
/>

### Keyword filtering with SearchText

Use **SearchText** to match any keywords or phrases embedded in your traces. It searches across span and resource attributes (including prompt/response fields), so you can:

- Track sensitive topics (e.g., “refund policy”, “PCI”, “unsafe”) as dedicated monitors
- Isolate incident-related traffic and watch the quality recover
- Run targeted evaluations for specific features, intents, or cohorts

This turns production monitoring into topic-level QA: you’re not just watching everything, you’re watching the parts that matter.

<DarkLightImage
  lightSrc="/images/monitors/monitor-filters.png"
  darkSrc="/images/monitors/monitor-filters.png"
  caption="Monitor options – sample & filter."
  alt="Sample and filter UI."
/>

## What happens after it runs

- Monitors sample recent AI spans using deterministic, hash‑based sampling (stable slices) and create a **Run**.
- Each sampled span is scored and appears inline on the **Traces** page with score chips; click any row to open the full trace.
- From a scored trace you can follow the link to the corresponding **Run** to see run‑level details.

## Where to view results

- **Traces**: Browse scored spans, filter by keywords with SearchText, and jump into details for debugging.
- **Runs**: See run history and performance over time, plus per‑run aggregates and plots on the run details pages.

<DarkLightImage
  lightSrc="/images/monitors/monitors-traces-score.png"
  darkSrc="/images/monitors/monitors-traces-score.png"
  caption="Monitor results – Scores."
  alt="Monitor results - traces and score UI."
/>

## Manage monitors

- **Edit** a monitor to change metrics, sampling, filters or toggle **Active**.
- **Delete** a monitor to stop processing entirely.

<DarkLightImage
  lightSrc="/images/monitors/monitor-edit.png"
  darkSrc="/images/monitors/monitor-edit.png"
  caption="Edit monitor modal."
  alt="Edit monitor modal screenshot."
/>

## Use cases

- Production monitoring of LLM quality and safety
- Auto-scoring on real user traffic
- Tracking model/prompt health over time

<Note>
Ready to set one up? Follow the [Online Evaluation & Production Monitoring Quickstart](/intro/production-monitoring-quickstart).
</Note>