---
title: "Tracing: Overview"
description: "Instrument and inspect every LLM request in minutes."
---

import { DarkLightImage } from "/snippets/dark-light-image.jsx";

<Info>
  New to Scorecard? Head straight to the [Tracing Quickstart](/intro/tracing-quickstart) or jump into our ready-to-run [Google Colab notebook](https://colab.research.google.com/github/scorecard-ai/scorecard-examples/blob/main/python-jupyter-openllmetry-openai/openllmetry_openai_example.ipynb) to see traces in under 5 minutes.
</Info>

LLM observability means knowing exactly what happened in every generationâ€”latency, token usage, prompts, completions, cost, errors and more. Scorecardâ€™s **Tracing** collects this data automatically via the open-source [OpenLLMetry](https://github.com/traceloop/openllmetry) SDK (Python & Node.js) and displays rich visualisations in the Scorecard UI.

## Why Tracing matters

- Debug long or failing requests in seconds.
- Audit prompts & completions for compliance and safety.
- Attribute quality and cost back to specific services or users.
- Feed production traffic into evaluations.

### If you call it something else

- **Observability / AI spans / request logs**: We capture standard OpenTelemetry traces and spans for LLM calls and related operations.
- **Agent runs / tools / function calls**: These appear as nested spans in the trace tree, with inputs/outputs when available.
- **Prompt/Completion pairs**: Extracted from common keys (`openinference.*`, `ai.prompt` / `ai.response`, `gen_ai.*`) so they can be turned into testcases and scored.

---

## Instrument once, capture everything

<CodeGroup>

```bash Python (pip)
pip install traceloop-sdk
```


```bash TypeScript/Node (npm)
npm install @traceloop/node-server-sdk
```

</CodeGroup>

Scorecard also supports standard OpenTelemetry (OTLP/HTTP) exporters across languages (Python, TypeScript/JS, Java, Go, .NET, Rust)â€”point your exporter at your Scorecard project and include your API key.

<CodeGroup>

```python Python
from traceloop.sdk import Traceloop
from traceloop.sdk.instruments import Instruments

# Initialize OpenLLMetry. Works with any supported provider (OpenAI, Anthropic, Gemini, Bedrock, etc.)
# Set scorecard.project_id to route traces to a specific project (defaults to oldest project)
Traceloop.init(
    disable_batch=True,
    instruments={Instruments.OPENAI},
    resource_attributes={
        "scorecard.project_id": "<your-project-id>"
    }
)

# Make any LLM/provider call with your client â€“ it will be traced automatically
```


```ts TypeScript/Node
import { Traceloop, Instruments } from '@traceloop/node-server-sdk'

// Initialize OpenLLMetry. Works with any supported provider or HTTP instrumentation
// Set scorecard.project_id to route traces to a specific project (defaults to oldest project)
Traceloop.init({
  disableBatch: true,
  instruments: new Set([Instruments.OPENAI]),
  resourceAttributes: {
    'scorecard.project_id': '<your-project-id>'
  }
})

// Make any LLM/provider call with your client â€“ it will be traced automatically
```

</CodeGroup>

See the [Quickstart](/intro/tracing-quickstart) for full environment variable setup, examples and best practices.

---

## Explore traces in Scorecard

<Frame caption="Traces dashboard â€“ search, filters, cost & scores">
  ![Traces View](/images/monitors/monitors-traces-search.png)
</Frame>

Scorecard automatically groups spans into traces and surfaces:

- **Timestamp & duration** â€“ when and how long the request ran.
- **Service & span tree** â€“ navigate nested function/tool calls with a collapsible span tree for easier navigation through long, highly nested traces.
- **Token & cost breakdown** â€“ estimate spend per trace via model pricing.
- **Scores column** â€“ if a trace links to an evaluation run the results appear inline (`TraceScoresCell`).
- **Full-text search & filters** â€“ search any span attribute (`searchText`) or limit to a specific project/time-range.
- **Copyable Trace ID** â€“ quickly copy and share trace identifiers.

All table controls map to URL parameters so you can share filtered trace views.

### Search & filters

- **Time ranges**: 30m, 24h, 3d, 7d, 30d, All.
- **Project scope**: toggle between Current project and All projects.
- **SearchText**: fullâ€‘text across span/resource attributes (including prompt/response fields).
- **Match previews**: quick context snippets with deep links to traces.
- **Cursor pagination**: efficient browsing with shareable URLs.

---

## Trace Grouping

When running batch operations or multi-step workflows, you can group related traces into a single run using the `scorecard.tracing_group_id` span attribute. This makes it easier to track and analyze workflows that span multiple LLM calls.

### How it works

Add the `scorecard.tracing_group_id` attribute to your spans with a shared identifier. Scorecard automatically groups spans with the same group ID into a single run.

<CodeGroup>

```python Python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

# Use the same group_id for all related operations
group_id = "batch-job-123"

with tracer.start_as_current_span("process_document") as span:
    span.set_attribute("scorecard.tracing_group_id", group_id)
    # Your LLM call here
    
with tracer.start_as_current_span("summarize_results") as span:
    span.set_attribute("scorecard.tracing_group_id", group_id)
    # Another LLM call in the same workflow
```

```typescript TypeScript
import { trace } from '@opentelemetry/api';

const tracer = trace.getTracer('my-service');

// Use the same groupId for all related operations
const groupId = 'batch-job-123';

const span1 = tracer.startSpan('process_document');
span1.setAttribute('scorecard.tracing_group_id', groupId);
// Your LLM call here
span1.end();

const span2 = tracer.startSpan('summarize_results');
span2.setAttribute('scorecard.tracing_group_id', groupId);
// Another LLM call in the same workflow
span2.end();
```

</CodeGroup>

### Use cases

- **Batch processing**: Group all documents processed in a single batch job
- **Multi-step agents**: Track all LLM calls within an agent's execution
- **Workflows**: Link related operations across different services
- **A/B testing**: Group traces by experiment variant for comparison

<Tip>
  Traces with the same `scorecard.tracing_group_id` will appear together in the same run, making it easy to analyze aggregate metrics across related operations.
</Tip>


---

## AI-Specific Error Detection

Scorecard's tracing goes beyond technical failures to detect AI-specific behavioral issues that traditional observability misses. The system acts as an always-on watchdog, analyzing every AI interaction to catch both obvious technical errors and subtle behavioral problems that could impact user experience.

### Silent Failure Detection

The most dangerous errors in AI systems are "silent failures" where your AI responds but incorrectly. Scorecard automatically detects behavioral errors including off-topic responses, workflow interruptions, safety violations, hallucinations, and context loss. These silent failures often go unnoticed without specialized AI observability but can severely impact user trust and application effectiveness.

Technical errors like rate limits, timeouts, and API failures are captured automatically through standard trace error recording. However, AI applications also face unique challenges like semantic drift, safety policy violations, factual accuracy issues, and task completion failures that require intelligent analysis beyond traditional error logging.

### Custom Error Detection

Create custom metrics through Scorecard's UI to detect application-specific behavioral issues. Design AI-powered metrics that analyze trace data for off-topic responses, safety violations, or task completion failures. These custom metrics automatically evaluate your traces and surface problematic interactions that would otherwise go unnoticed in production.

---

## Supported Frameworks & Providers

Scorecard traces LLM applications built with popular open-source frameworks through [OpenLLMetry](https://github.com/traceloop/openllmetry). OpenLLMetry provides automatic instrumentation for:

### Application Frameworks
- **CrewAI** â€“ Multi-agent collaboration
- **Haystack** â€“ Search and question-answering pipelines
- **LangChain** â€“ Chains, agents, and tool calls
- **Langflow** â€“ Visual workflow builder
- **LangGraph** â€“ Multi-step workflows and state machines
- **LiteLLM** â€“ Unified interface for 100+ LLMs
- **LlamaIndex** â€“ RAG pipelines and document retrieval
- **[OpenAI Agents SDK](https://github.com/openai/openai-agents-python?tab=readme-ov-file#tracing)** â€“ Assistants API and function calling
- **[Vercel AI SDK](https://ai-sdk.dev/providers/observability/scorecard)** â€“ Full-stack AI applications

<Tip>
  Scorecard is featured as a recommended observability provider in the official [Vercel AI SDK documentation](https://ai-sdk.dev/providers/observability/scorecard) and [OpenAI Agents Python README](https://github.com/openai/openai-agents-python?tab=readme-ov-file#tracing).
</Tip>

### LLM Providers
- Aleph Alpha
- Anthropic
- AWS Bedrock
- AWS SageMaker
- Azure OpenAI
- Cohere
- Google Gemini
- Google Vertex AI
- Groq
- HuggingFace
- IBM Watsonx AI
- Mistral AI
- Ollama
- OpenAI
- Replicate
- Together AI
- and more

### Vector Databases
- Chroma
- LanceDB
- Marqo
- Milvus
- Pinecone
- Qdrant
- Weaviate

<Info>
  For the complete list of supported integrations, see the [OpenLLMetry repository](https://github.com/traceloop/openllmetry). All integrations are built on OpenTelemetry standards and maintained by the community.
</Info>

### Custom Providers

For frameworks or providers not listed above, you can use:

- **HTTP Instrumentation**: OpenLLMetry's `instrument_http()` for HTTP-based APIs
- **Manual Spans**: Emit custom OpenTelemetry spans for proprietary systems

See the [OpenLLMetry documentation](https://www.traceloop.com/docs/openllmetry/getting-started-python) for manual instrumentation guides.

---

## Use cases

- **Production observability for LLM quality and safety**
- **Debugging slow/failed requests with full span context**
- **Auditing prompts/completions for compliance**
- **Attributing token cost and latency to services/cohorts**
- **Building evaluation datasets from real traffic (Trace to Testcase)**

---

## Next steps

1. Follow the [Quickstart](/intro/tracing-quickstart) to send your first trace.
2. Open the [Colab notebook](https://colab.research.google.com/github/scorecard-ai/scorecard-examples/blob/main/python-jupyter-openllmetry-openai/openllmetry_openai_example.ipynb) for an interactive tour.

Happy tracing\! ðŸš€