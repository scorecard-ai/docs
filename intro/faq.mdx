---
title: "Frequently Asked Questions"
sidebarTitle: "FAQ"
description: "Common questions about Scorecard's AI evaluation platform"
---

import { Accordion, AccordionGroup, Note, Warning, Info } from "@mintlify/components";

## Getting Started

<AccordionGroup>
  <Accordion title="What is an eval/evaluation?">
    An **evaluation** (or "eval") is the process of systematically testing and measuring your AI system's performance against a set of test cases. In Scorecard, an evaluation involves:
    
    - **Testset**: A collection of input/output pairs to test against
    - **System**: The AI model, prompt, or API endpoint being evaluated  
    - **Metrics**: Criteria used to score performance (accuracy, tone, safety, etc.)
    - **Run**: The execution of the evaluation across all test cases
    
    Think of it like unit testing for AI - you define what good looks like, run your system against test cases, and get quantitative scores to measure performance.
  </Accordion>

  <Accordion title="How does Scorecard differ from other AI evaluation tools?">
    Scorecard is designed for **production AI systems** with features like:
    
    - **Real-time monitoring**: Continuous evaluation of live AI applications
    - **Multi-turn conversations**: Test complex conversational flows
    - **Custom endpoints**: Evaluate any HTTP API, not just model APIs
    - **Trace integration**: Connect evaluation results to production traces
    - **Enterprise features**: SAML SSO, compliance, and advanced security
  </Accordion>

  <Accordion title="What programming languages does Scorecard support?">
    Scorecard provides SDKs for:
    - **Python**: Full-featured SDK with all capabilities
    - **JavaScript/TypeScript**: Complete Node.js and browser support
    - **REST API**: Universal HTTP access for any language
    - **Framework integrations**: LangChain, LlamaIndex, OpenAI, and more
  </Accordion>
</AccordionGroup>

## Limits and Constraints

<AccordionGroup>
  <Accordion title="What are the document and text limits in Scorecard?">
    Scorecard handles large-scale AI evaluation with the following limits:
    
    **Input/Output Text**:
    - **Maximum input size**: 1MB per test case input
    - **Maximum output size**: 1MB per test case output  
    - **Prompt length**: Up to 500KB per prompt version
    - **Context length**: Limited by your chosen model's context window
    
    **Documents and Files**:
    - **File upload**: Up to 100MB per file for testset imports
    - **Batch processing**: Up to 10,000 test cases per evaluation run
    - **API requests**: 50MB maximum payload size
    
    **Evaluation Runs**:
    - **Concurrent runs**: 10 simultaneous runs per organization
    - **Run duration**: 24-hour maximum execution time
    - **Test cases per run**: Up to 50,000 test cases
    
    <Note>
      For larger datasets or custom limits, contact enterprise support at enterprise@scorecard.io
    </Note>
  </Accordion>

  <Accordion title="Are there rate limits for API usage?">
    Yes, Scorecard implements rate limits to ensure platform stability:
    
    **API Rate Limits**:
    - **Standard Plan**: 1,000 requests per minute
    - **Pro Plan**: 5,000 requests per minute  
    - **Enterprise Plan**: Custom limits based on requirements
    
    **Evaluation Limits**:
    - **Free Plan**: 100 evaluations per month
    - **Standard Plan**: 1,000 evaluations per month
    - **Pro Plan**: 10,000 evaluations per month
    - **Enterprise Plan**: Unlimited evaluations
    
    Rate limit headers are included in all API responses for monitoring usage.
  </Accordion>

  <Accordion title="What is the playbook text limit?">
    **Sim Agent Playbooks** (persona instructions for multi-turn simulation) have the following limits:
    
    - **Maximum playbook length**: 50KB of text
    - **Template variables**: Up to 100 variables per playbook
    - **Conversation turns**: Maximum 50 turns per simulation (safety limit)
    - **Stop conditions**: Up to 10 custom stop conditions per simulation
    
    Playbooks support Jinja2 templating for dynamic content and can reference any field from your test cases.
  </Accordion>
</AccordionGroup>

## Features and Capabilities

<AccordionGroup>
  <Accordion title="How does metadata work in Scorecard?">
    **Metadata** in Scorecard allows you to store additional context without affecting evaluations:
    
    **Testcase Metadata**:
    - Mark fields as "metadata" in testset schemas
    - Stored with test cases but excluded from evaluation logic
    - Useful for tracking source, difficulty, categories, etc.
    
    **Trace Metadata**:
    - Custom attributes on spans (user_id, session_id, etc.)
    - Model parameters and configuration data
    - Performance metrics and timing information
    
    **Run Metadata**:
    - Git commit SHA, branch information
    - Environment details (staging, production)
    - Custom tags and labels for organization
    
    Example usage:
    ```json
    {
      "input": "What's the weather?",
      "expected_output": "I'll help you check the weather",
      "source": "customer_support_logs",  // metadata
      "difficulty": "easy",              // metadata
      "created_by": "data_team"          // metadata
    }
    ```
  </Accordion>

  <Accordion title="How is latency measured and reported?">
    Scorecard automatically captures **latency metrics** across your AI pipeline:
    
    **Measurement Points**:
    - **End-to-end latency**: Total request time from input to output
    - **Model inference time**: Time spent in model API calls
    - **Processing time**: Custom logic execution time
    - **Network latency**: Time spent in HTTP requests
    
    **Reporting**:
    - **Real-time dashboards**: Live latency monitoring
    - **Percentile analysis**: P50, P90, P95, P99 latency breakdown
    - **Trend analysis**: Latency over time with alerting
    - **Trace-level detail**: Individual request timing breakdowns
    
    **Alerting**:
    ```python
    # Set up latency alerts
    alert = client.alerts.create(
        name="High Latency Alert",
        metric="p95_latency",
        threshold=5000,  # 5 seconds
        window="10m"
    )
    ```
  </Accordion>

  <Accordion title="What types of AI systems can Scorecard evaluate?">
    Scorecard supports evaluation of any AI system accessible via API:
    
    **Model Types**:
    - **Large Language Models**: GPT, Claude, Llama, Gemini, etc.
    - **Embedding Models**: OpenAI, Cohere, custom embeddings
    - **Multimodal Models**: Vision, audio, and text processing
    - **Fine-tuned Models**: Custom models hosted anywhere
    
    **System Types**:
    - **RAG Systems**: Retrieval-augmented generation pipelines
    - **Chatbots**: Customer service, virtual assistants
    - **Tool-calling Agents**: Function calling and API integrations
    - **Custom APIs**: Any HTTP endpoint returning AI-generated content
    
    **Deployment Options**:
    - **Cloud APIs**: OpenAI, Anthropic, Google, AWS Bedrock
    - **Self-hosted**: Models running on your infrastructure  
    - **Hybrid**: Combination of cloud and on-premise systems
  </Accordion>
</AccordionGroup>

## Technical Details

<AccordionGroup>
  <Accordion title="How does Scorecard handle sensitive data and privacy?">
    Scorecard is built with **privacy by design** principles:
    
    **Data Security**:
    - **Encryption**: All data encrypted in transit (TLS 1.3) and at rest (AES-256)
    - **Access Controls**: Role-based permissions and organization isolation
    - **Audit Logging**: Complete audit trail of all data access
    - **Compliance**: SOC 2 Type II, GDPR, and enterprise compliance
    
    **Privacy Controls**:
    - **Data Redaction**: Automatic PII detection and masking
    - **Data Retention**: Configurable retention policies
    - **Right to Delete**: Complete data deletion capabilities
    - **Data Residency**: Control where your data is stored
    
    See our [Privacy by Design](/features/privacy-by-design) documentation for complete details.
  </Accordion>

  <Accordion title="Can I run Scorecard evaluations offline or on-premise?">
    Scorecard offers flexible deployment options:
    
    **Cloud Service** (Recommended):
    - Fully managed service at app.scorecard.io
    - Automatic updates and maintenance
    - Global CDN and high availability
    
    **Enterprise On-Premise**:
    - Self-hosted deployment in your infrastructure
    - Complete data sovereignty and control
    - Custom integrations with internal systems
    - Available for Enterprise customers
    
    **Hybrid Approach**:
    - Evaluation logic runs on-premise
    - Results optionally synced to cloud dashboard
    - Best of both worlds for security-sensitive organizations
    
    Contact enterprise@scorecard.io for on-premise deployment options.
  </Accordion>

  <Accordion title="How do I migrate from other evaluation tools?">
    Scorecard provides migration support for common evaluation platforms:
    
    **Data Migration**:
    - Import existing test datasets (CSV, JSON, JSONL)
    - Convert evaluation metrics to Scorecard format
    - Migrate historical evaluation results
    
    **Common Migrations**:
    - **From custom scripts**: Convert to Scorecard SDK calls
    - **From academic benchmarks**: Import MMLU, HellaSwag, etc.
    - **From other platforms**: Bulk export/import workflows
    
    **Migration Assistance**:
    - Free migration consultation for Enterprise customers
    - Custom scripts for complex data transformations
    - Parallel running during transition period
    
    Contact our support team at support@scorecard.io for personalized migration assistance.
  </Accordion>
</AccordionGroup>

## Billing and Plans

<AccordionGroup>
  <Accordion title="How does Scorecard pricing work?">
    Scorecard uses **evaluation-based pricing** with transparent costs:
    
    **Free Plan**:
    - 100 evaluations per month
    - Basic metrics and reporting
    - Community support
    
    **Standard Plan** ($49/month):
    - 1,000 evaluations per month
    - Advanced metrics and custom scoring
    - Email support
    
    **Pro Plan** ($199/month):
    - 10,000 evaluations per month
    - SAML SSO (up to 25 users)
    - Priority support
    - Advanced integrations
    
    **Enterprise Plan** (Custom):
    - Unlimited evaluations
    - Custom deployment options
    - Dedicated support and CSM
    - Custom compliance features
    
    Additional evaluation packages available for all plans.
  </Accordion>

  <Accordion title="What counts as an evaluation?">
    An **evaluation** is counted each time Scorecard scores a single test case:
    
    **Examples**:
    - 1 test case × 1 metric = 1 evaluation
    - 1 test case × 3 metrics = 3 evaluations  
    - 100 test cases × 2 metrics = 200 evaluations
    
    **Not Counted**:
    - Viewing existing results
    - Creating/editing test cases
    - Monitoring and tracing (separate feature)
    - API calls for data retrieval
    
    **Bulk Discounts**: Enterprise customers get volume discounts for large-scale evaluations.
  </Accordion>
</AccordionGroup>

## Related Resources

<Card title="Getting Started" href="/intro/sdk-quickstart">
  Quick start guide for new users
</Card>

<Card title="Contact Support" href="https://www.scorecard.io/book-a-demo">
  Get help with setup and migration
</Card>

<Card title="Status Page" href="https://status.scorecard.io">
  Real-time platform status and uptime
</Card>