---
title: "Metrics Quickstart"
sidebarTitle: "Metrics"
description: "Create metrics, group them, run evaluations, and read scores."
---

import { Note, Card } from "@mintlify/components";
import { DarkLightImage } from "/snippets/dark-light-image.jsx";

    <DarkLightImage
      lightSrc="/images/metrics/metrics-overview-light.png"
      darkSrc="/images/metrics/metrics-overview-dark.png"
      caption="Metrics page with Metrics, Groups, and Templates tabs."
      alt="Metrics page overview"
    />

Metrics define what “good” looks like for your LLM. You describe the criteria (e.g., helpfulness, groundedness, safety), and Scorecard turns that into repeatable scores you can track across runs and over time.

<Steps>
  <Step title="Open Metrics and explore templates">
    Go to your project’s **Metrics** page. Start fast by copying a proven template, then tailor the guidelines to your domain.

    <DarkLightImage
      lightSrc="/images/metrics/metrics-templates-light.png"
      darkSrc="/images/metrics/metrics-templates-dark.png"
      caption="Templates list with Create from Template."
      alt="Metric templates list"
    />

    <DarkLightImage
      lightSrc="/images/metrics/metrics-template-light.png"
      darkSrc="/images/metrics/metrics-template-dark.png"
      caption="Template details with description and output type."
      alt="Template details panel"
    />
  </Step>

  <Step title="Create your first metric (AI‑scored)">
    You can also create a metric from scratch. Provide a name, description, clear <strong>guidelines</strong>, and choose an <strong>Evaluation Type</strong> and <strong>Output Type</strong>.

    <DarkLightImage
      lightSrc="/images/metrics/metrics-create-a-light.png"
      darkSrc="/images/metrics/metrics-create-a-dark.png"
      caption="New Metric modal – name, description, evaluation type, and guidelines."
      alt="New metric configuration modal"
    />

    For AI‑scored metrics, pick the evaluator model and keep temperature low for repeatability.

    <DarkLightImage
      lightSrc="/images/metrics/metrics-create-b-light.png"
      darkSrc="/images/metrics/metrics-create-b-dark.png"
      caption="AI evaluator settings and output type."
      alt="AI evaluation settings"
    />


    <DarkLightImage
      lightSrc="/images/metrics/metrics-outputtype-light.png"
      darkSrc="/images/metrics/metrics-outputtype-dark.png"
      caption="Choose Boolean (pass/fail) or Integer (1–5)."
      alt="Output type options"
    />

    <Note>
      <strong>Guidelines matter.</strong> Describe what to reward and what to penalize, and include 1–2 concise examples if helpful. These instructions become the core of the evaluator prompt.
    </Note>
  </Step>

  <Step title="Optional: Human‑scored metric">
    Some criteria are best judged by humans. Select <strong>Human</strong> as the evaluation type and write clear instructions for reviewers.

    <DarkLightImage
      lightSrc="/images/metrics/metrics-create-human-light.png"
      darkSrc="/images/metrics/metrics-create-human-dark.png"
      caption="Human evaluation – provide instructions for reviewers."
      alt="Human evaluation configuration"
    />
  </Step>

  <Step title="Create a Metric Group">
    Group related metrics (e.g., Groundedness + Relevance + Safety) so you can select them together during a run.

    <DarkLightImage
      lightSrc="/images/metrics/metrics-create-group-light.png"
      darkSrc="/images/metrics/metrics-create-group-dark.png"
      caption="Create a Metric Group and select metrics to include."
      alt="Create metric group modal"
    />

    After saving, you’ll see your groups alongside individual metrics.

    <DarkLightImage
      lightSrc="/images/metrics/metric-groups-light.png"
      darkSrc="/images/metrics/metrics-groups-dark.png"
      caption="Metric Groups overview."
      alt="Metric groups list"
    />
  </Step>

  <Step title="Kick off a Run and pick metrics">
    Start a run from your project dashboard, Playground, or Runs list. In the <strong>Kickoff Run</strong> modal, choose a Testset, select your System or Endpoint, and add one or more <strong>Metrics</strong> or a <strong>Metric Group</strong>.

    <ul>
      <li>Pick individual metrics by name, or select a Metric Group for consistency.</li>
      <li>Integer metrics show an average on the run; Boolean metrics show pass‑rate.</li>
      <li>Runs with human‑scored metrics will show status <em>Scoring</em> until reviewers submit results.</li>
    </ul>

    See <a className="underline" href="/features/runs">Runs & Results</a> for a deeper walkthrough of the kickoff modal and options.

    When the run executes and scoring completes, open the run to review results.

    <DarkLightImage
      lightSrc="/images/metrics/metrics-run-light.png"
      darkSrc="/images/metrics/metrics-run-dark.png"
      caption="Run results with per‑record scores."
      alt="Run results table"
    />
  </Step>

  <Step title="Inspect scores and explanations">
    Hover over or click a score to view the evaluator’s reasoning and properties. This helps validate and tune your guidelines.

    <DarkLightImage
      lightSrc="/images/metrics/metrics-scores-detail-light.png"
      darkSrc="/images/metrics/metrics-scores-detail-dark.png"
      caption="Score explanation with reasoning and model details."
      alt="Score explanation panel"
    />

    Drill into any record to see inputs, outputs, expected outputs, and all scores together.

    <DarkLightImage
      lightSrc="/images/metrics/metrics-testrecord-light.png"
      darkSrc="/images/metrics/metrics-testrecord-dark.png"
      caption="Record view showing inputs/outputs and metric results."
      alt="Test record details"
    />
  </Step>
</Steps>

## Metric types

- **AI‑scored**: Uses a model to apply your guidelines consistently and at scale.
- **Human‑scored**: Great for nuanced judgments or gold‑standard baselines.
- **Heuristic (SDK)**: Deterministic, code‑based checks via the SDK (e.g., latency, regex, policy flags).
- **Output types**: Choose <strong>Boolean</strong> (pass/fail) or <strong>Integer (1–5)</strong>.

## Second‑party metrics (optional)

If you already use established evaluation libraries, you can mirror those metrics in Scorecard:

- **MLflow genai**: Relevance, Answer Relevance, Faithfulness, Answer Correctness, Answer Similarity
- **RAGAS**: Faithfulness, Answer Relevancy, Context Recall, Context Precision, Context Relevancy, Answer Semantic Similarity

Copy a matching template, then tailor the guidelines to your product domain.

## Best practices for strong metrics

- **Be specific.** Minimize ambiguity in guidelines; include “what not to do.”
- **Pick the right output type.** Use Boolean for hard requirements; 1–5 for nuance.
- **Keep temperature low.** Use ≈0 for repeatable AI scoring.
- **Pilot and tighten.** Run on 10–20 cases, then refine wording to reduce false positives.
- **Bundle into groups.** Combine complementary checks (e.g., Relevance + Faithfulness + Safety) to keep evaluations consistent.

<Note>
  Looking for vetted, ready‑to‑use metrics? Explore <a href="/features/best-in-class-metrics" className="underline">Best‑in‑Class Metrics</a> and copy templates (including MLflow and RAGAS). You can also create deterministic checks via the SDK using <strong>Heuristic</strong> metrics.
</Note>

## Related resources

<Card title="Runs" icon="play" href="/features/runs">
  Create and analyze evaluations
</Card>

<Card title="A/B Comparison" icon="git-merge" href="/features/a-b-comparison">
  Compare two runs side‑by‑side
</Card>

<Card title="Monitoring" icon="activity" href="/features/monitoring">
  Continuously score production traffic
</Card>

<Card title="Best‑in‑Class Metrics" icon="star" href="/features/best-in-class-metrics">
  Explore curated, proven metrics
</Card>

<Card title="API Reference" icon="code" href="/api-reference/create-metric">
  Create metrics via API
</Card>