---
title: "Online Evaluation & Production Monitoring Quickstart"
description: "Start scoring live production traffic with Scorecard in minutes."
mode: "center"
---

import { DarkLightImage } from '/snippets/dark-light-image.jsx';

Scorecard can evaluate live requests as they flow through your application. This guide picks up right after the **Tracing Quickstart** and shows how to enable continuous evaluation on your live traffic.

<Info>
Monitors work like observability dashboards, but with one key difference: Scorecard also runs **evaluations/auto‑scoring** on sampled traces so you see quality metrics over time, not just system metrics.
</Info>

<Tip>
Already finished the <a href="/intro/tracing-quickstart">Tracing Quickstart</a>? Use the same project and skip to **Configure a Monitor** below.
</Tip>

## Steps
<Steps>
  <Step title="Instrument tracing (one-time)">
    Production monitoring relies on traces. Follow the <a href="/intro/tracing-quickstart">Tracing Quickstart</a> to connect your service and start sending spans. Once traces flow to your project, come back here to turn on continuous evaluation.
  </Step>

  <Step title="Configure a Monitor">
    In the Scorecard UI open **Monitors → New Monitor**.

    1. Select the **project** that receives production traces (same one used in the tracing quickstart).
    2. Pick the **metrics** you want to run continuously (toxicity, factuality, latency, etc.).
    3. Set **frequency** (1m, 5m, 30m, 1h, 1d).
    4. Choose a **sample rate** (1 %–100 %).
    5. Add **filters** (by <code>spanName</code>, <code>serviceName</code>, or free-text **searchText**).
    6. Toggle **Active** to start/stop without deleting.

<DarkLightImage
  lightSrc="/images/monitors/monitor-filters.png"
  darkSrc="/images/monitors/monitor-filters.png"
  caption="Monitor options – sample & filter."
  alt="Sample and filter UI."
/>

    **How sampling works**

    - Deterministic, hash-based sampling on `TraceId` gives stable slices across runs.
    - Large projects are automatically down-sampled according to your rate.

<DarkLightImage
  lightSrc="/images/monitors/monitor-sample-rate.png"
  darkSrc="/images/monitors/monitor-sample-rate.png"
  caption="Monitor options – sample & filter."
  alt="Sample and filter UI."
/>

    **Filters**

    - Narrow evaluation by span name, service name, or free-text search across span/resource attributes.
    - Use **SearchText** to target keywords and phrases that appear anywhere in a trace (including prompts and responses). Examples: “refund policy”, “PCI”, “unsafe content”, “billing migration”. This lets you monitor a topic, incident, or cohort end-to-end and measure quality improvements precisely where they matter.

<DarkLightImage
  lightSrc="/images/monitors/monitor-filters-2.png"
  darkSrc="/images/monitors/monitor-filters-2.png"
  caption="Monitor options – sample & filter."
  alt="Sample and filter UI."
/>

    **Where prompts & completions come from**

    Scorecard looks for common attributes: `openinference.*`, `ai.prompt` / `ai.response`, and `gen_ai.*`. Custom keys can be mapped in the UI.

    <Note>
    Your traces should follow the same conventions used by OpenInference / gen_ai ecosystems (e.g., Traceloop, OpenLLMetry). If you don’t see monitoring or scoring results:

    - Inspect a span in the **Traces** page and verify it contains prompt and completion under keys like `openinference.input` / `openinference.output`, `ai.prompt` / `ai.response.*`, or `gen_ai.prompt.*` / `gen_ai.completion.*`.
    - Ensure those spans are within your monitor’s filters (service/span/searchText) and sampling window.
    Once the attributes are present, Scorecard will automatically extract and score them.
    </Note>
  </Step>

  <Step title="Where results show up">
    Each cycle, Scorecard samples recent AI spans, extracts inputs/outputs, creates a **Run**, and auto‑scores **Records** with your metrics.

    - On **Traces**: you’ll see scored spans inline; click a row to view the full trace and follow the link to its run.
    - On **Runs**: you’ll find run history and per‑run aggregates (plots and stats) for deeper analysis.

  <DarkLightImage
    lightSrc="/images/monitors/monitors-traces-search.png"
    darkSrc="/images/monitors/monitors-traces-search.png"
    caption="Traces search page with scores created by a 'monitor'."
    alt="Screenshot of traces page."
  />
  </Step>
</Steps>

## What’s next?

- Promote interesting spans with **Trace ➜ Testcase** to build datasets from production usage.
- Compare results in **Runs & Results** when iterating on models or prompts.

<Note>
Looking for an end-to-end example? Check the open-source <a href="https://github.com/scorecard-ai/scorecard-examples" target="_blank">production monitoring sample app</a>.
</Note>