---
title: "Quickstart"
description: "Evaluate a simple LLM system with Scorecard in minutes."
mode: "center"
---

Scorecard has [Python](https://pypi.org/project/scorecard-ai), [JavaScript](https://www.npmjs.com/package/scorecard-ai), and [Go](https://github.com/scorecard-ai/scorecard-go) SDKs. If you're using Python, you can follow along in [Google Colab](https://colab.research.google.com/drive/1A53j48o75Decrjz7WzDVP62XeDEKCHHP).

## Steps

<Steps>
  <Step title="Setup accounts">
    Create a [Scorecard account](https://app.getscorecard.ai/dashboard) and an [OpenAI account](https://platform.openai.com), then set your [Scorecard API key](https://app.getscorecard.ai/settings) and [OpenAI API key](https://platform.openai.com/api-keys) as environment variables.

    ```sh
    export SCORECARD_API_KEY="your_scorecard_api_key"
    export OPENAI_API_KEY="your_openai_api_key"
    ```
  </Step>

  <Step title="Install SDKs">
    Install the Scorecard and OpenAI libraries:

    <CodeGroup>
      ```sh Python (pip install)
      pip install --pre scorecard-ai
      pip install openai
      ```

      ```sh JavaScript (npm install)
      npm install scorecard-ai@alpha
      npm install openai
      ```

      ```go Go (go get)
      go get github.com/scorecard-ai/scorecard-go
      go get github.com/scorecard-ai/scorecard-go/option
      go get github.com/openai/openai-go
      ```
    </CodeGroup>
  </Step>

  <Step title="Create simple LLM system">
    Create a simple LLM system to evaluate. When using Scorecard, systems use dictionaries as input and output.
    
    For the quickstart, the LLM system is `run_system()`, which translates the user's message to a different tone.
    
    `input["original"]` is the user's message and `input["tone"]` is the tone to translate to. The output is a dictionary containing the translated message (`rewritten`).

    <CodeGroup>
      ```py Python
      from openai import OpenAI
      
      # By default, the API key is taken from the environment variable.
      openai = OpenAI()
      
      # Example:
      # run_system({"original": "Hello, world!", "tone": "formal"})
      # -> {"rewritten": "Greetings, world."}
      def run_system(system_input: dict) -> dict:
          response = openai.responses.create(
              model="gpt-4o-mini",
              instructions=f"You are a tone translator. Convert the user's message to the tone: {system_input['tone']}",
              input=system_input['original'],
          )
          return {
              "rewritten": response.output_text
          }
      ```

      ```js JavaScript
      import OpenAI from 'openai';

      // By default, the API key is taken from the environment variable.
      const openai = new OpenAI();
      
      // Example:
      // runSystem({"original": "Hello, world!", "tone": "formal"})
      // -> {"rewritten": "Greetings, world."}
      async function runSystem(systemInput) {
        const response = await openai.responses.create({
          model: 'gpt-4o-mini',
          instructions: `You are a tone translator. Convert the user's message to the tone: ${systemInput.tone}`,
          input: systemInput.original,
        });
        return {
          rewritten: response.output_text
        };
      }
      ```
    </CodeGroup>
  </Step>

  <Step title="Setup Scorecard">
    <CodeGroup>
      ```py Python
      from scorecard_ai import Scorecard

      # By default, the API key is taken from the environment variable.
      scorecard = Scorecard()
      ```

      ```js JavaScript
      import Scorecard from 'scorecard-ai';

      // By default, the API key is taken from the environment variable.
      const scorecard = new Scorecard();
      ```
    </CodeGroup>
  </Step>
  
  <Step title="Create Project">
    Create a Project in Scorecard. This will be where your tests and runs will be stored. Copy the Project ID for later.

    <CodeGroup>
      ```py Python
      PROJECT_ID = "310"  # Replace with your project ID
      ```

      ```js JavaScript
      const PROJECT_ID = "310"; // Replace with your Project ID
      ```
    </CodeGroup>
  </Step>
  
  <Step title="Create Testset with Testcases">
    Create a Testset in the Project and add some Testcases. A Testset is a collection of Testcases used to evaluate the performance of an LLM system. A Testcase is a single input and ideal output pair that is used for scoring. Copy the Testset ID for later.

    Run this code to create a Testset with a schema matching our tone translator app.

    <CodeGroup>
      ```py Python [expandable]
      # Create a Testset with a schema matching our use case
      testset = scorecard.testsets.create(
          project_id=PROJECT_ID,
          name="Tone rewriter testset",
          description="Testcases about rewriting messages in a different tone.",
          field_mapping={
              # Inputs represent the input to the system.
              "inputs": ["original", "tone"],
              # Labels represent the expected output of the system.
              "labels": ["idealRewritten"],
              # Metadata fields are used for grouping Testcases, but not seen by the system.
              "metadata": [],
          },
          json_schema={
              "type": "object",
              "properties": {
                  # The original message.
                  "original": {"type": "string"},
                  # The tone that the message should be rewritten in.
                  "tone": {"type": "string"},
                  # The ideal AI-generated rewritten message.
                  "idealRewritten": {"type": "string"},
              },
              "required": ["original", "tone", "idealRewritten"],
          },
      )

      # Add Testcases matching the Testset's schema to the Testset
      scorecard.testcases.create(
          testset_id=testset.id,
          items=[
              {
                  "json_data": {
                      "original": "We need your feedback on the new designs ASAP.",
                      "tone": "polite",
                      "idealRewritten": "Hi, your feedback is crucial to the success of the new designs. Please share your thoughts as soon as possible.",
                  },
              },
              {
                  "json_data": {
                      "original": "I'll be late to the office because my cat is sleeping on my keyboard.",
                      "tone": "funny",
                      "idealRewritten": "Hey team! My cat's napping on my keyboard and I'm just waiting for her to give me permission to leave. I'll be a bit late!",
                  },
              },
              {
                  "json_data": {
                      "original": "Schedule a meeting to discuss this project.",
                      "tone": "casual",
                      "idealRewritten": "Let's find a time to chat about the project. Coffee or boba?",
                  },
              },
          ],
      )

      TESTSET_ID = testset.id
      ```

      ```js JavaScript [expandable]
      // Create a Testset with a schema matching our use case
      const testset = await scorecard.testsets.create(PROJECT_ID, {
        name: "Tone rewriter testset",
        description: "Testcases about rewriting messages in a different tone.",
        fieldMapping: {
          // Inputs represent the input to the system.
          inputs: ["original", "tone"],
          // Labels represent the expected output of the system.
          labels: ["idealRewritten"],
          // Metadata fields are used for grouping Testcases, but not seen by the system.
          metadata: [],
        },
        jsonSchema: {
          type: "object",
          properties: {
            original: { type: "string" },
            tone: { type: "string" },
            idealRewritten: { type: "string" },
          },
          required: ["original", "tone", "idealRewritten"],
        },
      });

      // Add Testcases matching the Testset's schema
      await scorecard.testcases.create(testsetId, {
        items: [
          {
            jsonData: {
              original: 'We need your feedback on the new designs ASAP.',
              tone: 'polite',
              idealRewritten:
                'Hi, your feedback is crucial to the success of the new designs. Please share your thoughts as soon as possible.',
            },
          },
          {
            jsonData: {
              original: "I'll be late to the office because my cat is sleeping on my keyboard.",
              tone: 'funny',
              idealRewritten:
                "Hey team! My cat's napping on my keyboard and I'm just waiting for her to give me permission to leave. I'll be a bit late!",
            },
          },
          {
            jsonData: {
              original: "Schedule a meeting to discuss this project.",
              tone: 'casual',
              idealRewritten:
                "Let's find a time to chat about the project. Coffee or boba?",
            },
          },
        ],
      });

      const TESTSET_ID = testset.id;
      ```
    </CodeGroup>
  </Step>
  
  <Step title="Create Metrics">
    From the Scorecard UI, create an AI-graded Metric named "Tone accuracy" to evaluate the tone translator system on. Copy the Metric ID for later.

    <Frame caption="Creating a Metric in the Scorecard UI." height="500">
      <img className="block dark:hidden" height="100" src="/images/quickstart/create-metric-light.png" alt="Screenshot of creating a 'Tone accuracy' metric." />
      <img className="hidden dark:block" height="100" src="/images/quickstart/create-metric-dark.png" alt="Screenshot of creating a 'Tone accuracy' metric." />
    </Frame>

    <CodeGroup>
      ```py Python
      METRIC_IDS = ["987"]  # Replace with your Metric ID
      ```

      ```js JavaScript
      const METRIC_IDS = ["987"]; // Replace with your Metric ID
      ```
    </CodeGroup>
  </Step>

  <Step title="Evaluate system">
    Run the system against the Testset and Metrics you've created and record the results in Scorecard.

    <CodeGroup>
      ```py Python
      from scorecard_ai.lib import run_and_evaluate

      run = run_and_evaluate(
          client=scorecard,
          project_id=PROJECT_ID,
          testset_id=TESTSET_ID,
          metric_ids=METRIC_IDS,
          system=lambda input: run_system(input)
      )

      print(f'Go to {run.url} and click "Run Scoring" to grade your Run.')
      ```

      ```js JavaScript
      const run = await runAndEvaluate(scorecard, {
          projectId: PROJECT_ID,
          testsetId: TESTSET_ID,
          metricIds: METRIC_IDS,
          system: runSystem,
      });
      console.log(`Go to ${run.url} and click "Run Scoring" to grade your Run.`);
      ```
    </CodeGroup>
  </Step>

  <Step title="Run scoring">
    Click the link in the output above, or find the Run in the Scorecard UI. On the Run page, click the "Run Scoring" button to score your system using the Metric you created.
  </Step>

  <Step title="Analyze results">
    Finally, review the results in Scorecard to understand the performance of the tone translator system.

    <Frame caption="Viewing results in the Scorecard UI." height="500">
      <img className="block dark:hidden" height="100" src="/images/quickstart/scoring-done-light.png" alt="Screenshot of viewing results in the Scorecard UI." />
      <img className="hidden dark:block" height="100" src="/images/quickstart/scoring-done-dark.png" alt="Screenshot of viewing results in the Scorecard UI." />
    </Frame>
  </Step>
</Steps>