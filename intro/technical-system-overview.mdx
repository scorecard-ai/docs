---
title: "Technical System Overview"
description: "How Scorecard works under the hood - architecture, data flow, and integration points for engineers"
---

## Architecture Overview

Scorecard is built on four core primitives that power all LLM testing workflows:

<Frame caption="Scorecard Core Architecture">
![Scorecard Core Architecture](/images/technical-system/1.png)
</Frame>

<Info>
**For engineers:** This guide covers Scorecard's technical implementation. For a product overview, see [What is Scorecard?](/intro/what-is-scorecard)
</Info>

## Product System Architecture

<Frame caption="Scorecard Product System Architecture">
![Scorecard Product System Architecture](/images/technical-system/product-system-architecture.png)
</Frame>

This diagram illustrates the complete Scorecard product architecture:

- **Requirements**: Define metrics and evaluation criteria
- **Test Management**: Organize test datasets and trial settings
- **Configuration**: Set up model templates and prompt parameters
- **Evaluation System**: Run simulations, trials, and analyze metrics
- **Results**: View scorecards and run history
- **Monitoring & Continuous Eval**: Track live performance and user feedback
- **Improvement**: Get AI-powered suggestions for optimization

The system creates a continuous feedback loop, where monitoring results feed back into requirements and test management for ongoing improvement.

## Core Primitives

<Tabs>
  <Tab title="Testsets" icon="database">
    ### Testsets
    Collections of test cases representing real-world scenarios.
    
    ```typescript
    interface Testset {
      id: string
      name: string
      cases: TestCase[]
      metadata: {
        version: string
        created_at: string // ISO 8601 format
        tags: string[]
      }
    }
    
    interface TestCase {
      id: string
      input: {
        prompt: string
        context?: string
        parameters?: Record<string, any>
      }
      expected?: {
        output?: string
        criteria?: EvaluationCriteria[] // Custom evaluation rules
      }
    }
    ```
    
    **Storage:** PostgreSQL with version control
    **Access:** REST API, SDK, UI
  </Tab>
  <Tab title="Runs" icon="play">
    ### Runs
    Executions of testsets against your LLM application.
    
    ```typescript
    interface Run {
      id: string
      testset_id: string
      status: 'pending' | 'running' | 'completed' | 'failed'
      config: {
        model: string
        temperature: number
        max_tokens: number
        system_prompt?: string
        // ... other model params
      }
      results: RunResult[] // Test execution results
      metadata: {
        triggered_by: 'api' | 'ui' | 'ci'
        environment: string
        commit_sha?: string
      }
    }
    ```
    
    **Execution:** Async job queue (Redis + workers)
    **Storage:** S3 for outputs, PostgreSQL for metadata
  </Tab>
  <Tab title="Metrics" icon="chart-bar">
    ### Metrics
    Evaluation functions that score LLM outputs.
    
    ```python
    from typing import Optional
    from dataclasses import dataclass
    
    @dataclass
    class Score:
        value: float  # Score between 0 and 1
        reasoning: str  # Explanation for the score
    
    class Metric:
        def evaluate(self, 
                    input: str,  # User input/prompt
                    output: str,  # LLM generated response
                    expected: Optional[str] = None) -> Score:  # Expected output (optional)
            # Implementation of metric evaluation logic
            pass
    
    # Pre-built metrics
    metrics = [
        "relevance",      # Output addresses the input
        "coherence",      # Logical flow and clarity
        "factuality",     # Accuracy of claims
        "safety",         # No harmful content
        "helpfulness",    # Useful for the user
    ]
    
    # Custom metric example
    class ToneMetric(Metric):
        def evaluate(self, input, output, expected=None):
            # Check if output matches desired tone
            return Score(value=0.95, reasoning="Professional tone")
    ```
    
    **Evaluation:** Python workers with GPU support
    **Caching:** Redis for repeated evaluations
  </Tab>
  <Tab title="Systems" icon="server">
    ### Systems
    Your LLM application endpoints and configurations.
    
    ```typescript
    interface System {
      id: string
      name: string
      type: 'api' | 'sdk' | 'playground'
      endpoint?: string
      auth?: {
        type: 'bearer' | 'api_key' | 'oauth'
        credentials: string  // AES-256 encrypted
      }
      default_config: ModelConfig // Default model parameters
      rate_limits: {
        requests_per_minute: number
        tokens_per_minute: number
      }
    }
    ```
    
    **Integration:** REST API, gRPC, WebSockets
    **Security:** End-to-end encryption, key rotation
  </Tab>
</Tabs>

## Implementation Workflow

<Steps>
  <Step title="Data Preparation" icon="database">
    <Frame caption="Ingesting Testcases into Scorecard">
    ![Ingesting Testcases into Scorecard](/images/technical-system/2.png)
    </Frame>
    
    **Best practices:**
    - Start with production logs
    - Include edge cases
    - Version control testsets
    - Tag by feature/risk level
  </Step>
  
  <Step title="Execution" icon="play">
    <Frame caption="Executing Your LLM App With Scorecard">
    ![Executing Your LLM App With Scorecard](/images/technical-system/3.png)
    </Frame>
    
    **Execution modes:**
    - **Playground**: Manual testing
    - **SDK**: Development workflow
    - **CI/CD**: Automated checks
    - **Monitoring**: Production traffic
  </Step>
  
  <Step title="Evaluation" icon="chart-line">
    <Frame caption="Scoring Your LLM App With Scorecard">
    ![Scoring Your LLM App With Scorecard](/images/technical-system/4.png)
    </Frame>
    
    **Evaluation stack:**
    1. Rule-based checks (fast)
    2. LLM-as-judge (accurate)
    3. Human review (ground truth)
    4. Meta-evaluation (validation)
  </Step>
</Steps>

## Developer Resources

<CardGroup cols={2}>
  <Card title="API Reference" icon="code" href="/api-reference">
    Complete API documentation with examples
  </Card>
  <Card title="Integration Guide" icon="book" href="/how-to-use-scorecard/trigger-run-with-github-actions">
    GitHub Actions and CI/CD setup
  </Card>
  <Card title="Example Apps" icon="github" href="https://github.com/scorecard-ai/examples">
    Reference implementations
  </Card>
  <Card title="Status Page" icon="signal" href="https://status.scorecard.ai">
    Real-time system status
  </Card>
</CardGroup>

<Note>
**Questions?** Engineering support available at engineering@scorecard.ai or in #engineering on [Slack](https://scorecard.io/slack).
</Note>
