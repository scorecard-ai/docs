---
title: "Technical System Overview"
description: "How Scorecard works under the hood - architecture, data flow, and integration points for engineers"
---

## Architecture Overview

Scorecard is built on four core primitives that power all LLM testing workflows:

<Frame caption="Scorecard Core Architecture">
![Scorecard Core Architecture](/images/technical-system/1.png)
</Frame>

<Info>
**For engineers:** This guide covers Scorecard's technical implementation. For a product overview, see [What is Scorecard?](/intro/what-is-scorecard)
</Info>

## Product System Architecture

<Frame caption="Scorecard Product System Architecture">
![Scorecard Product System Architecture](/images/technical-system/product-system-architecture.png)
</Frame>

This diagram illustrates the complete Scorecard product architecture:

- **Requirements**: Define metrics and evaluation criteria
- **Test Management**: Organize test datasets and trial settings
- **Configuration**: Set up model templates and prompt parameters
- **Evaluation System**: Run simulations, trials, and analyze metrics
- **Results**: View scorecards and run history
- **Monitoring & Continuous Eval**: Track live performance and user feedback
- **Improvement**: Get AI-powered suggestions for optimization

The system creates a continuous feedback loop, where monitoring results feed back into requirements and test management for ongoing improvement.

## Core Primitives

<Tabs>
  <Tab title="Testsets" icon="database">
    ### Testsets
    Collections of test cases representing real-world scenarios.
    
    ```typescript
    interface Testset {
      id: string
      name: string
      cases: TestCase[]
      metadata: {
        version: string
        created_at: string // ISO 8601 format
        tags: string[]
      }
    }
    
    interface TestCase {
      id: string
      inputs: Record<string, any> // Domain-specific inputs (e.g., {original: string, tone: string})
      expected?: Record<string, any> // Domain-specific expected outputs (e.g., {idealRewritten: string})
    }
    ```
    
    **Storage:** PostgreSQL with version control
    **Access:** REST API, SDK, UI
  </Tab>
  <Tab title="Runs" icon="play">
    ### Runs
    Executions of testsets against your LLM application.
    
    ```typescript
    interface Run {
      id: string
      url: string // Link to view results in UI
      testset_id: string
      status: 'pending' | 'running' | 'completed' | 'failed'
      project_id: string
      created_at: string // ISO 8601 format
      // Additional metadata varies by implementation
    }
    ```
    
    **Execution:** Async job queue (Redis + workers)
    **Storage:** S3 for outputs, PostgreSQL for metadata
  </Tab>
  <Tab title="Metrics" icon="chart-bar">
    ### Metrics
    Evaluation functions that score LLM outputs.
    
    ```python
    # Create metrics using the Scorecard API
    metric = scorecard.metrics.create(
        project_id=PROJECT_ID,
        name="Response Quality",
        eval_type="ai",  # AI-powered evaluation
        output_type="int",  # Score as integer (1-5)
        prompt_template="Rate the helpfulness and accuracy of this response from 1-5.\n\nInput: {{inputs.question}}\nOutput: {{output.response}}\nExpected: {{expected.response}}"
    )
    
    # Built-in metric types
    eval_types = [
        "ai",           # LLM-as-a-judge evaluation
        "exact_match",  # Exact string comparison
        "contains",     # Check if output contains expected text
        "regex",        # Regular expression matching
        "custom"        # Custom evaluation logic
    ]
    
    # Example: Domain-specific metric
    medical_metric = scorecard.metrics.create(
        project_id=PROJECT_ID,
        name="Medical Accuracy",
        eval_type="ai",
        output_type="int",
        prompt_template="Evaluate if this medical advice follows clinical guidelines. Rate 1-5."
    )
    ```
    
    **Evaluation:** Python workers with GPU support
    **Caching:** Redis for repeated evaluations
  </Tab>
  <Tab title="Systems" icon="server">
    ### Systems
    Your LLM application endpoints and configurations.
    
    ```typescript
    interface System {
      id: string
      name: string
      type: 'api' | 'sdk' | 'playground'
      endpoint?: string
      auth?: {
        type: 'bearer' | 'api_key' | 'oauth'
        credentials: string  // AES-256 encrypted
      }
      default_config: ModelConfig // Default model parameters
      rate_limits: {
        requests_per_minute: number
        tokens_per_minute: number
      }
    }
    ```
    
    **Integration:** REST API, gRPC, WebSockets
    **Security:** End-to-end encryption, key rotation
  </Tab>
</Tabs>

## Implementation Workflow

<Steps>
  <Step title="Data Preparation" icon="database">
    <Frame caption="Ingesting Testcases into Scorecard">
    ![Ingesting Testcases into Scorecard](/images/technical-system/2.png)
    </Frame>
    
    **Best practices:**
    - Start with production logs
    - Include edge cases
    - Version control testsets
    - Tag by feature/risk level
  </Step>
  
  <Step title="Execution" icon="play">
    <Frame caption="Executing Your LLM App With Scorecard">
    ![Executing Your LLM App With Scorecard](/images/technical-system/3.png)
    </Frame>
    
    **Execution modes:**
    - **Playground**: Manual testing
    - **SDK**: Development workflow
    - **CI/CD**: Automated checks
    - **Monitoring**: Production traffic
  </Step>
  
  <Step title="Evaluation" icon="chart-line">
    <Frame caption="Scoring Your LLM App With Scorecard">
    ![Scoring Your LLM App With Scorecard](/images/technical-system/4.png)
    </Frame>
    
    **Evaluation stack:**
    1. Rule-based checks (fast)
    2. LLM-as-judge (accurate)
    3. Human review (ground truth)
    4. Meta-evaluation (validation)
  </Step>
</Steps>

## Developer Resources

<CardGroup cols={2}>
  <Card title="API Reference" icon="code" href="/api-reference">
    Complete API documentation with examples
  </Card>
  <Card title="Integration Guide" icon="book" href="/how-to-use-scorecard/trigger-run-with-github-actions">
    GitHub Actions and CI/CD setup
  </Card>
  <Card title="Example Apps" icon="github" href="https://github.com/scorecard-ai/examples">
    Reference implementations
  </Card>
  <Card title="Status Page" icon="signal" href="https://status.scorecard.ai">
    Real-time system status
  </Card>
</CardGroup>

<Note>
**Questions?** Engineering support available at engineering@scorecard.ai or in #engineering on [Slack](https://scorecard.io/slack).
</Note>
