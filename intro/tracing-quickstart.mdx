---
title: "Tracing Quickstart"
description: "Trace your LLM applications by pointing your client to Scorecard."
mode: "center"
---

import { DarkLightImage } from '/snippets/dark-light-image.jsx';

This quickstart shows how to trace your LLM applications with Scorecard. Change your `baseURL` to `llm.scorecard.io` - no SDKs, no dependency management, works with your existing code. Supports OpenAI, Anthropic, and streaming responses.

<Info>
  **Using Vercel AI SDK?** Check out our [AI SDK Wrapper](/features/ai-sdk-wrapper) for automatic tracing with zero manual instrumentation.
</Info>

<Info>
  **Need more control?** See the SDK wrappers section below for custom spans and deeper integration, or use OpenTelemetry directly.
</Info>

## Steps

<Steps>
  <Step title="Get your Scorecard API key">
    Create a [Scorecard account](https://app.scorecard.io) and grab your API key from [Settings](https://app.scorecard.io/settings).

    ```bash
    export SCORECARD_API_KEY="ak_..."
    export OPENAI_API_KEY="sk_..."  # or ANTHROPIC_API_KEY
    ```
  </Step>
  <Step title="Point your client to Scorecard">
    Change the `baseURL` to `https://llm.scorecard.io`. Everything else in your code stays the same - your LLM calls will be automatically traced.

    <CodeGroup>

    ```js JavaScript
    import OpenAI from 'openai';
    
    const client = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
      baseURL: 'https://llm.scorecard.io',  // Add this line
      defaultHeaders: {
        'x-scorecard-api-key': process.env.SCORECARD_API_KEY,  // Add this
        'x-scorecard-project-id': 'my-chatbot'  // Optional: organize traces by project
      }
    });
    
    // Use OpenAI normally - everything is automatically traced!
    const response = await client.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: 'Hello!' }]
    });
    
    // For Claude/Anthropic, use the same pattern:
    // import Anthropic from '@anthropic-ai/sdk';
    // const client = new Anthropic({
    //   apiKey: process.env.ANTHROPIC_API_KEY,
    //   baseURL: 'https://llm.scorecard.io',
    //   defaultHeaders: { 'x-scorecard-api-key': process.env.SCORECARD_API_KEY }
    // });
    ```

    
    ```py Python
    from openai import OpenAI
    
    client = OpenAI(
        api_key=os.environ["OPENAI_API_KEY"],
        base_url="https://llm.scorecard.io",  # Add this line
        default_headers={
            "x-scorecard-api-key": os.environ["SCORECARD_API_KEY"],  # Add this
            "x-scorecard-project-id": "my-chatbot"  # Optional: organize traces by project
        }
    )
    
    # Use OpenAI normally - everything is automatically traced!
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Hello!"}]
    )
    
    # For Claude/Anthropic, use the same pattern:
    # from anthropic import Anthropic
    # client = Anthropic(
    #     api_key=os.environ["ANTHROPIC_API_KEY"],
    #     base_url="https://llm.scorecard.io",
    #     default_headers={"x-scorecard-api-key": os.environ["SCORECARD_API_KEY"]}
    # )
    ```

    </CodeGroup>
  </Step>
  <Step title="View traces in Scorecard">
    Run your application, then visit [app.scorecard.io](https://app.scorecard.io) and navigate to **Traces**.

    You'll see full request/response data, token usage, latency, and errors for all your LLM calls. Streaming responses are captured too.

    <DarkLightImage lightSrc="/images/tracing-light.png" darkSrc="/images/tracing-dark.png" caption="Viewing traces in the Scorecard UI." alt="Screenshot of viewing traces in the Scorecard UI." />
  </Step>
</Steps>

## How It Works

Scorecard forwards your requests to the LLM provider while capturing telemetry:

```
Your App → llm.scorecard.io → OpenAI/Anthropic
              ↓
         Scorecard (traces, metrics, costs)
```

Your LLM calls work the same way - Scorecard forwards requests and captures telemetry.

## Need Deeper Integration?

For more control over tracing with custom spans, workflows, and tasks:

**Scorecard SDK Wrappers:**

<CodeGroup>

```py Python
from openai import OpenAI
from scorecard_ai import wrap

# Wrap your client for automatic tracing
openai = wrap(OpenAI(api_key=os.getenv("OPENAI_API_KEY")))

# Use normally - all calls are traced
response = openai.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)
```


```js JavaScript
import { wrap } from 'scorecard-ai';
import OpenAI from 'openai';

// Wrap your client for automatic tracing
const openai = wrap(new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
}));

// Use normally - all calls are traced
const response = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello!' }]
});
```

</CodeGroup>

**Or use OpenTelemetry directly** for full control over instrumentation.

## Examples & Learn More

- [Node.js Examples](https://github.com/scorecard-ai/scorecard-node/tree/main/examples) - OpenAI, Anthropic, Vercel AI SDK, nested traces
- [Python Examples](https://github.com/scorecard-ai/scorecard-python/tree/main/examples) - OpenAI, Anthropic, nested traces
- [More Examples](https://github.com/scorecard-ai/scorecard-examples) - Additional integration examples
- [Tracing Features](/features/tracing) - Advanced tracing patterns