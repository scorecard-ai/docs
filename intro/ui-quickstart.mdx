---
title: 'UI Quickstart'
description: 'Evaluate a simple LLM agent with Scorecard in minutes.'
---

import { DarkLightImage } from '/snippets/dark-light-image.jsx';
import { Note } from '@mintlify/components';

Kick off your first run in minutes using the Example Project that’s created automatically when you sign up. You can also browse the run that was kicked off for you and explore how testsets, prompts, and metrics fit together.

<Steps>
  <Step title="Open the Example Project and click Kickoff Run">
    After org creation, navigate to the Example Project's [Records page](/features/records). Click the **Kickoff Run** button in the top right to open the Kickoff Run modal.

    <DarkLightImage
      lightSrc="/images/records-kickoff-run-button-light.png"
      darkSrc="/images/records-kickoff-run-button-dark.png"
      caption="Kickoff Run button on the Records page."
      alt="Screenshot showing the Kickoff Run button in the top right of the Records page."
    />
  </Step>

  <Step title="Kick off your first run">
    In the modal, you can use the default selected Testset, System, and Metrics.

    Click "Kickoff run" to create the run and automatically evaluate the system.

    <DarkLightImage
      lightSrc="/images/example-project/kickoff-modal-pre-filled-light.png"
      darkSrc="/images/example-project/kickoff-modal-pre-filled-dark.png"
      caption="Kickoff Run modal."
      alt="Kickoff modal with prefilled Testset, Prompt, and Metrics selections."
    />
  </Step>

  <Step title="View results">
    After your run starts and scoring completes, open the results to see per-record scores, distributions, and explanations.

    <DarkLightImage
      lightSrc="/images/example-project/run-detail-with-example-run-scores-light.png"
      darkSrc="/images/example-project/run-detail-with-example-run-scores-dark.png"
      caption="Run details with scores and aggregates."
      alt="Run details page with scores and aggregates."
    />

    <Tip>
      Click the <strong>Run Again</strong> button in the top right corner to iterate with a different prompt version, model, or metric set.
    </Tip>

    Click any record in the [Records page](/features/records) to view individual testcase inputs, outputs, and score explanations.
  </Step>

  <Step title="Browse the Example Project">
    Learn how the sample data is organized:
    
    <Note>
      <strong>Quick tour of the data</strong>
      <ul>
        <li><strong>Tone Testset</strong>: inputs <code>original</code>, <code>tone</code> → expected <code>idealRewritten</code>.</li>
        <li><strong>Prompt versions</strong> for Tone — already set to Scorecard Cloud with low temperature for consistency.</li>
        <li><strong>Metrics</strong>: Correctness (AI, 1-5), Human Tone Check (Human, Boolean).</li>
      </ul>
    </Note>
    
    <DarkLightImage
      lightSrc="/images/example-project/example-project-testsets-overview-light.png"
      darkSrc="/images/example-project/example-project-testsets-overview-dark.png"
      caption="Testsets overview."
      alt="Testsets overview"
    />

    Open a Testset to see its schema and Testcases. Click a testcase row to view its inputs and expected outputs.

    <DarkLightImage
      lightSrc="/images/example-project/example-project-testset-light.png"
      darkSrc="/images/example-project/example-project-testset-dark.png"
      caption="A sample Testset with Testcases."
      alt="Testset details"
    />

    Next, browse Prompts. Use “View” to open a prompt, review messages, and model settings.

    <DarkLightImage
      lightSrc="/images/example-project/example-prompts-light.png"
      darkSrc="/images/example-project/example-prompts-dark.png"
      caption="Prompts list."
      alt="Prompts list"
    />

    Inside a prompt version, see the template (Jinja-style variables) and evaluator model configuration.

    <DarkLightImage
      lightSrc="/images/example-project/example-project-prompt-light.png"
      darkSrc="/images/example-project/example-project-prompt-dark.png"
      caption="Prompt version with template and model settings."
      alt="Prompt version editor"
    />

    Finally, explore Metrics to learn how scoring works. Each metric has guidelines, evaluation type, and output type.

    <DarkLightImage
      lightSrc="/images/example-project/example-project-metrics-light.png"
      darkSrc="/images/example-project/example-project-metrics-dark.png"
      caption="Metrics configured for the Example Project."
      alt="Metrics list."
    />
  </Step>
</Steps>

## Where to go next

- Read about creating and managing Testsets in [Testsets](/features/testsets)
- Dive deeper into running evaluations in [Runs & Results](/features/runs)
- Explore interactive prompt iteration in the [Playground](/features/playground)
- Define and reuse evaluation criteria with [Metrics](/intro/metrics-quickstart)

That's it — you've seen Scorecard in action and how example data flows through prompts, runs, and metrics. Have fun iterating!