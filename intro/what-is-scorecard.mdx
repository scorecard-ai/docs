---
title: "What Is Scorecard?"
description: "Build trusted AI agents with systematic testing and evaluation"
icon: "rocket"
---

<Frame>
![Scorecard Workflow](/images/what-is-scorecard/scorecard-workflow.gif)
</Frame>

Scorecard is an AI evaluation platform that helps teams build reliable AI products through systematic AI evals. Test your AI systems before they impact users, catch regressions early, and deploy with confidence.

## Why you need AI evals

Building production AI systems without proper AI evals is risky. Teams often discover issues in production because they lack visibility into AI behavior across different scenarios. Manual evals don't scale, and without systematic evaluation, it's impossible to know if changes improve or degrade performance.

Scorecard provides the infrastructure to run AI evals systematically, validate improvements, and prevent regressions.

## What Scorecard provides

**Testset management** — Convert real production scenarios into reusable test cases. When your AI fails in production, capture that case and add it to your regression suite.

**Playground evaluation** — Test prompts and models side-by-side without writing code. Compare different approaches across providers (OpenAI, Anthropic, Google Gemini) to find what works best.

**Domain-specific metrics** — Choose from pre-validated metrics for your industry or create custom evaluators. Available for legal, financial services, healthcare, customer support, and general quality evaluation.

**Automated workflows** — Integrate AI evals into your CI/CD pipeline. Get alerts when performance drops and prevent regressions before they reach users.

## How it works

1. **Create testsets** from your real use cases and edge cases
2. **Run evaluations** across different prompts, models, and configurations  
3. **Compare results** to identify the best performing approaches
4. **Deploy with confidence** knowing your AI system meets quality standards

```python
# Example: Create and evaluate a testset
testset = scorecard.create_testset(
    name="customer-support-cases",
    cases=production_scenarios
)

# Define custom metrics for your domain
metric = scorecard.create_metric(
    name="response-quality",
    description="Evaluate helpfulness and accuracy"
)

# Run evaluation across your test cases
scorecard.evaluate(testset, metric)
```

## Who uses Scorecard

**AI Engineers** run evals systematically instead of manually checking outputs

**Product Teams** validate that AI behavior matches user expectations

**QA Teams** build comprehensive test suites for AI systems

**Leadership** gets visibility into AI reliability and performance

## Getting started

<Steps>
  <Step title="Try the playground">
    Test prompts interactively without writing code
  </Step>
  <Step title="Create your first testset">
    Add test cases that represent your real use cases
  </Step>
  <Step title="Run evaluations">
    Compare different approaches and measure performance
  </Step>
  <Step title="Integrate with CI/CD">
    Automate AI evals to catch regressions early
  </Step>
</Steps>

<CardGroup cols={2}>
  <Card title="5-minute quickstart" icon="play" href="/intro/quickstart">
    Set up your first evaluation
  </Card>
  <Card title="Try the playground" icon="flask" href="/features/playground">
    Start testing without code
  </Card>
</CardGroup>

## Next steps

Ready to integrate Scorecard into your workflow? We provide SDK support for Python and TypeScript, full REST API access, and GitHub Actions integration.

See our [Technical System Overview](/intro/technical-system-overview) for architecture details, or email support@scorecard.io for help getting started.