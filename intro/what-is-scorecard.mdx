---
title: "What Is Scorecard?"
description: "The simulation platform for building frontier AI agents. Run thousands of realistic scenarios in minutes and ship new capabilities with confidence."
---

<Frame>
![Scorecard Workflow](/images/what-is-scorecard/scorecard-workflow.gif)
</Frame>

Scorecard is the simulation platform for AI agent self-improvement. Run your agents through thousands of realistic scenarios in minutes, encode expert judgment into scalable reward models, and ship frontier capabilities in days.

## The bottleneck is feedback, not building

Teams are building increasingly complex agents — multi-step workflows, consequential actions, real-world integrations. But the way most teams validate these agents hasn't kept up. The current approach is manual: review a handful of production cases, wait weeks for expert feedback, and hope nothing slips through.

This limits you to scenarios you've already seen. Edge cases stay hidden until they hit production. Expert time doesn't scale — every new capability means more review cycles, longer iteration loops, and slower releases.

The bottleneck has shifted from building to feedback. Scorecard flips this by turning expert judgment into automated reward models and replacing manual review with large-scale simulation.

<Tip>
**Traditional approach:** Review tens of production cases over weeks.

**Scorecard approach:** Simulate tens of thousands of scenarios in 30 minutes.
</Tip>

## How Scorecard works

<Steps>
  <Step title="Encode expert judgment">
    Define reward criteria in natural language. Scorecard turns them into automated judges that score every scenario consistently and at scale.

    [Learn about metrics →](/features/metrics)
  </Step>
  <Step title="Simulate at scale">
    Run your agent through thousands of realistic scenarios using AI-powered personas. Generate diverse test scenarios automatically — no manual case writing required.

    [Multi-turn simulation →](/features/multi-turn-simulation) · [Synthetic data generation →](/features/synthetic-data-generation)
  </Step>
  <Step title="Compare and improve">
    Quantitative A/B comparison across every metric. Iterate visually in the Playground with real-time feedback to find the best prompt, model, or architecture.

    [A/B comparison →](/features/a-b-comparison) · [Playground →](/features/playground)
  </Step>
  <Step title="Ship with confidence">
    Integrate simulation into CI/CD so every pull request is validated automatically. Monitor production with tracing and feed real traffic back into your simulation suite.

    [GitHub Actions →](/features/github-actions) · [Tracing →](/features/tracing)
  </Step>
</Steps>

## What you can do

<CardGroup cols={2}>
  <Card title="Test multi-turn agents" icon="comments" href="/features/multi-turn-simulation">
    Simulate full conversations with AI-powered personas that behave like real users.
  </Card>
  <Card title="Encode quality standards" icon="scale-balanced" href="/features/metrics">
    Turn expert judgment into automated reward models that score every scenario.
  </Card>
  <Card title="Generate test scenarios" icon="wand-magic-sparkles" href="/features/synthetic-data-generation">
    Create thousands of diverse, realistic test cases automatically.
  </Card>
  <Card title="Compare agent versions" icon="code-compare" href="/features/a-b-comparison">
    Run quantitative A/B comparisons across every metric you care about.
  </Card>
  <Card title="Iterate visually" icon="flask" href="/features/playground">
    Test prompts and models side-by-side with real-time feedback.
  </Card>
  <Card title="Monitor production" icon="chart-line" href="/features/tracing">
    Capture every step of agent execution and feed real traffic back into simulations.
  </Card>
</CardGroup>

## Works with your agent stack

<CardGroup cols={3}>
  <Card title="Claude Agent SDK" icon="bolt" href="/intro/claude-agent-sdk-tracing">
    Zero-code tracing. Set three environment variables and get full visibility into agent decisions, tool use, and costs.
  </Card>
  <Card title="LangChain" icon="link" href="/intro/langchain-quickstart">
    Trace LangChain agents and chains with OpenTelemetry.
  </Card>
  <Card title="Any LLM" icon="plug" href="/intro/tracing-quickstart">
    Works with OpenAI, Anthropic, Google, and any OpenTelemetry-compatible provider.
  </Card>
</CardGroup>

## Built by simulation engineers

Scorecard was built by engineers from Waymo, Uber, and SpaceX — teams that used large-scale simulation to ship autonomous vehicles, global logistics, and rockets. We're applying the same principles to AI agents: simulate exhaustively, measure rigorously, and ship with confidence.

## Get started

<CardGroup cols={3}>
  <Card title="Run your first evaluation" icon="play" href="/intro/sdk-quickstart">
    Set up Scorecard and run a simulation in minutes.
  </Card>
  <Card title="Try the Playground" icon="flask" href="/features/playground">
    Start testing without writing code.
  </Card>
  <Card title="Talk to our team" icon="calendar" href="https://www.scorecard.io/book-a-demo">
    Book a demo and see Scorecard in action.
  </Card>
</CardGroup>

Email support@scorecard.io for help getting started.
