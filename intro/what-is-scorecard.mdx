---
title: "What Is Scorecard?"
description: "Build trusted AI agents with systematic testing and evaluation"
---

<Frame>
![Scorecard Workflow](/images/what-is-scorecard/scorecard-workflow.gif)
</Frame>

Scorecard is an AI evaluation platform that helps teams build reliable AI products through systematic AI evals. Test your AI agents before they impact users, catch regressions early, and deploy with confidence.

## Why you need AI evals

Building production AI agents without proper AI evals is risky. Teams often discover issues in production because they lack visibility into AI behavior across different scenarios. Manual evals don't scale, and without systematic evaluation, it's impossible to know if changes improve or degrade performance.

Scorecard provides the infrastructure to run AI evals systematically, validate improvements, and prevent regressions.

## Who uses Scorecard

**AI Engineers** run evals systematically instead of manually checking outputs.

**Agent Developers** test multi-turn conversations and complex workflows.

**Product Teams** validate that AI behavior matches user expectations.

**QA Teams** build comprehensive test suites for AI agents.

**Leadership** gets visibility into AI reliability and performance.

## What Scorecard provides

**[Tracing](/features/tracing)** — Capture and inspect every step of your AI agent's execution. Understand how your agent processes requests, identify bottlenecks, and debug failures with full visibility into each trace.

**[Domain-specific metrics](/features/metrics)** — Choose from pre-validated metrics for your industry or create custom evaluators, available for legal, financial services, healthcare, customer support, and general quality evaluation.

**[Testset management](/features/testsets)** — Convert real production scenarios into reusable test cases. When your AI fails in production, capture that case and add it to your regression suite.

**[Playground evaluation](/features/playground)** — Test prompts and models side-by-side without writing code. Compare different approaches across providers (OpenAI, Anthropic, Google Gemini) to find what works best.

**[Automated workflows](/features/github-actions)** — Integrate AI evals into your CI/CD pipeline. Get alerts when performance drops and prevent regressions before they reach users.

## How it works

1. **Instrument your agent** with Scorecard's tracing SDK to capture every step of execution.
2. **Define metrics** that evaluate quality, accuracy, and safety of your agent's outputs.
3. **Analyze traces** to identify failures, bottlenecks, and areas for improvement.
4. **Deploy with confidence**, knowing your AI agent meets quality standards.


<CardGroup cols={2}>
  <Card title="5-minute quickstart" icon="play" href="/intro/quickstart">
    Set up your first evaluation
  </Card>
  <Card title="Try the playground" icon="flask" href="/features/playground">
    Start testing without code
  </Card>
</CardGroup>

## Next steps

Ready to integrate Scorecard into your workflow? We provide SDK support for Python and TypeScript, full REST API access, and GitHub Actions integration.

Email support@scorecard.io for help getting started.