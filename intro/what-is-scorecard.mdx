---
title: "What Is Scorecard?"
description: "Build trusted AI agents with systematic testing and evaluation"
icon: "rocket"
---

<Frame>
![Scorecard Workflow](/images/what-is-scorecard/scorecard-workflow.gif)
</Frame>

Scorecard is an AI performance evaluation platform that helps teams build more reliable AI products. By creating a fast feedback loop for testing and improvement, Scorecard gives you transparency into your AI system's behavior before it impacts users.

## The problem we solve

Building production AI systems is challenging:

- **Slow feedback cycles** make it hard to iterate quickly
- **Lack of visibility** into how your AI performs in real scenarios  
- **No systematic testing** leads to issues discovered in production
- **Organizational silos** create blind spots in AI development

Scorecard provides the infrastructure to test smarter, validate the right metrics, and continuously improve your AI products.

## How to use Scorecard

### 1. Create and manage test cases

Convert real production scenarios into reusable test sets. When your AI fails in production, capture that case and add it to your regression suite.

```python
# Create a testset from production failures
testset = scorecard.create_testset(
    name="customer-support-edge-cases",
    cases=production_failures
)
```

### 2. Compare models and prompts

Use the playground to prototype and evaluate different approaches side-by-side. Test variations across multiple providers (OpenAI, Anthropic, Google Gemini) to find what works best.

**To compare prompts:**
1. Enter your test inputs in the playground
2. Configure different prompt versions
3. Run evaluations across all variations
4. View results side-by-side to identify the best performer

### 3. Evaluate with domain-specific metrics

Choose from pre-validated metrics for your industry or create custom evaluators by describing what you want to measure.

**Available metric domains:**
- Legal and compliance
- Financial services  
- Healthcare
- Customer support
- General quality (relevance, factuality, safety)

**To create custom metrics:**
```python
# Define what matters for your use case
custom_metric = scorecard.create_metric(
    name="medical-accuracy",
    description="Evaluate if medical advice follows guidelines"
)
```

### 4. Deploy tested configurations

Once you've identified high-performing prompts through rigorous testing, deploy them to production with confidence.

**Deployment workflow:**
1. Test extensively in Scorecard
2. Export validated prompt configuration
3. Deploy to your production system
4. Monitor performance (real-time monitoring coming soon)

### 5. Build a regression suite

Transform production issues into test cases that prevent future regressions.

**To build your test suite:**
1. Log production AI interactions
2. Identify failures or suboptimal responses
3. Add these cases to your testset
4. Run regression tests before each deployment

## Key capabilities

### Live observability
Monitor how your AI system interacts with users in real-time. Trace requests, responses, and performance metrics to understand system behavior.

### Prompt versioning
Store and track all prompt iterations. Compare performance across versions and roll back if needed.

### A/B testing
Run controlled experiments to validate improvements before full deployment. Test different models, prompts, or parameters with statistical rigor.

### Human validation
When automated metrics aren't enough, use human labeling to establish ground truth for complex evaluations.

## Who benefits from Scorecard

- **AI Engineers**: Test systematically instead of manually checking outputs
- **Product Teams**: Validate AI behavior matches user expectations
- **QA Teams**: Build comprehensive test suites for AI systems
- **Leadership**: Get visibility into AI reliability and performance

## Getting started

<Steps>
  <Step title="Set up your workspace">
    Create test sets that represent your real use cases
  </Step>
  <Step title="Configure evaluations">
    Choose metrics that matter for your domain
  </Step>
  <Step title="Run tests">
    Evaluate your AI system across all test cases
  </Step>
  <Step title="Iterate and improve">
    Use insights to refine prompts and configurations
  </Step>
</Steps>

<CardGroup cols={2}>
  <Card title="Try the playground" icon="play" href="/features/playground">
    Start testing without writing code
  </Card>
  <Card title="Read the quickstart" icon="book" href="/intro/quickstart">
    Set up your first evaluation in 5 minutes
  </Card>
</CardGroup>

## Technical integration

For engineering teams ready to integrate Scorecard into their workflow:

- **SDK support**: Python and TypeScript libraries available
- **API access**: Full REST API for custom integrations
- **CI/CD integration**: GitHub Actions and deployment pipelines
- **Tracing**: OpenTelemetry-compatible logging

See our [Technical System Overview](/intro/technical-system-overview) for architecture details.

## Learn more

- **Documentation**: Browse our guides and API reference
- **Community**: Join [Slack](https://scorecard.io/slack) to connect with other teams
- **Support**: Email support@scorecard.ai for help getting started